{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3_Sols.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW1_j_rLzjo1"
      },
      "source": [
        "# OpenNMT Tutorial and Starter Code\r\n",
        "(modified from the OpenNMT quickstart to work in Colab)\r\n",
        "\r\n",
        "While creating your own models from scratch is common for many tasks, often times it's useful to rely on a tool or framework to aid in this. In this exercise we're going to look at one popular NMT tool, OpenNMT, as a way to use beam search, which could be tricky to implement efficiently on your own.\r\n",
        "\r\n",
        "Finally we'll look at how to configure different models for OpenNMT including Transformer, which we'll look at in detail next week.\r\n",
        "\r\n",
        "OpenNMT, is similar to other ML frameworks in that it relies on a combination of editable .yaml files and command line tools to run the training procedure.  \r\n",
        "### Make sure you have the toy-ende.yml from the lab repository.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KLPZyf4B_L"
      },
      "source": [
        "#### Due to some colab compatibility issues we will use a different version of torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R24Vt0AksrhX",
        "outputId": "5e52a71b-86ce-4c51-9b1c-8cab3c246e93"
      },
      "source": [
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.6.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (708.0MB)\n",
            "\u001b[K     |████████████████████████████████| 708.0MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.7.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.6.0+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.6.0+cu101 torchvision-0.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZmSKOA4JJh"
      },
      "source": [
        "### Next let's get OpenNMT as well as a toy English to German corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2NZIg4rnuUW",
        "outputId": "792c8eb0-dd05-4dff-b175-c2e3e759c19c"
      },
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git\r\n",
        "!cd OpenNMT-py; pip install -e .\r\n",
        "!wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\r\n",
        "!tar xf toy-ende.tar.gz\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 17082 (delta 1), reused 1 (delta 0), pack-reused 17071\u001b[K\n",
            "Receiving objects: 100% (17082/17082), 272.99 MiB | 38.04 MiB/s, done.\n",
            "Resolving deltas: 100% (12318/12318), done.\n",
            "Obtaining file:///content/OpenNMT-py\n",
            "Collecting tqdm<5,>=4.51\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/3e/2730d0effc282960dbff3cf91599ad0d8f3faedc8e75720fdf224b31ab24/tqdm-4.59.0-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (1.6.0+cu101)\n",
            "Collecting torchtext==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.9MB/s \n",
            "\u001b[?25hCollecting configargparse<2,>=1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c3/17846950db4e11cc2e71b36e5f8b236a7ab2f742f65597f3daf94f0b84b7/ConfigArgParse-1.4.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (2.4.1)\n",
            "Requirement already satisfied: flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (1.1.2)\n",
            "Collecting waitress==1.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hCollecting pyyaml==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 52.4MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/63/17c6ac0d8a0cfa5ff7257e52edb6759d12dc266392f6c97f5c65c0c7238c/pyonmttok-1.25.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py==2.0.1) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py==2.0.1) (0.16.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.0.1) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.0.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.27.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.32.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (54.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.4.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.36.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py==2.0.1) (1.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.1.0)\n",
            "Building wheels for collected packages: configargparse, pyyaml\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.4-cp37-none-any.whl size=19638 sha256=e28836066ea1bc225ee71c40f27576ffbac0028b0d0d4e517db7993e6773140d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/61/f7/626bbd080a9f2f70015f92025e0af663c595146083f3d9aa05\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=5ed7502d833f9202c7339cf3546fe384ca86ea445fc485b88590d79b37c94fb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built configargparse pyyaml\n",
            "Installing collected packages: tqdm, sentencepiece, torchtext, configargparse, waitress, pyyaml, pyonmttok, OpenNMT-py\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torchtext 0.9.0\n",
            "    Uninstalling torchtext-0.9.0:\n",
            "      Successfully uninstalled torchtext-0.9.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for OpenNMT-py\n",
            "Successfully installed OpenNMT-py configargparse-1.4 pyonmttok-1.25.0 pyyaml-5.3.1 sentencepiece-0.1.95 torchtext-0.5.0 tqdm-4.59.0 waitress-1.4.4\n",
            "--2021-03-16 01:36:48--  https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.136.190\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.136.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1662081 (1.6M) [application/x-gzip]\n",
            "Saving to: ‘toy-ende.tar.gz’\n",
            "\n",
            "toy-ende.tar.gz     100%[===================>]   1.58M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-03-16 01:36:48 (28.4 MB/s) - ‘toy-ende.tar.gz’ saved [1662081/1662081]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKXbe2KJ4Uqr"
      },
      "source": [
        "## Processing Vocab\r\n",
        "\r\n",
        "Once we have the corpus and OpenNMT we can build the vocab we'll use. This relies on having a config file with this information laid out.\r\n",
        "\r\n",
        "Let's take a second to look at the config file we'll be using toy-ende.yml, which you should upload to Colab using the file upload on the left.\r\n",
        "\r\n",
        "The important part of the data processing are in the top parts of the yaml file:\r\n",
        "\r\n",
        "```\r\n",
        "# toy_en_de.yaml\r\n",
        "\r\n",
        "## Where the samples will be written\r\n",
        "save_data: toy-ende/run/example\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: toy-ende/run/example.vocab.src\r\n",
        "tgt_vocab: toy-ende/run/example.vocab.tgt\r\n",
        "# Prevent overwriting existing files in the folder\r\n",
        "overwrite: False\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "    corpus_1:\r\n",
        "        path_src: toy-ende/src-train.txt\r\n",
        "        path_tgt: toy-ende/tgt-train.txt\r\n",
        "    valid:\r\n",
        "        path_src: toy-ende/src-val.txt\r\n",
        "        path_tgt: toy-ende/tgt-val.txt\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "We specify where the data is, where to save it, as well as the vocab files corresponding to the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvhOxc0XoC-Z",
        "outputId": "cade5505-fc7b-4a2d-caf9-9c5489d70022"
      },
      "source": [
        "!onmt_build_vocab -config toy-ende.yml -n_sample 10000\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-03-15 22:37:40,227 INFO] Counter vocab from 10000 samples.\n",
            "[2021-03-15 22:37:40,227 INFO] Build vocab on 10000 transformed examples/corpus.\n",
            "[2021-03-15 22:37:40,236 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-03-15 22:37:40,237 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:37:40,538 INFO] Counters src:24995\n",
            "[2021-03-15 22:37:40,538 INFO] Counters tgt:35816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyDiOm9Q5Fiz"
      },
      "source": [
        "## Training\r\n",
        "\r\n",
        "Next we will beging training with OpenNMT, again using the same config file, however, below we'll look at the relevant parts:\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "# Train on a single GPU\r\n",
        "world_size: 1\r\n",
        "gpu_ranks: [0]\r\n",
        "\r\n",
        "# Where to save the checkpoints\r\n",
        "# Note it won't actually make it to 10,000 steps because of early stopping\r\n",
        "save_model: toy-ende/run/model\r\n",
        "save_checkpoint_steps: 500\r\n",
        "train_steps: 10000\r\n",
        "valid_steps: 500\r\n",
        "early_stopping: 2\r\n",
        "\r\n",
        "\r\n",
        "# Checkpoint settings\r\n",
        "keep_checkpoint: 3\r\n",
        "seed: 531\r\n",
        "warmup_steps: 400\r\n",
        "report_every: 100\r\n",
        "\r\n",
        "# Model (note these are actually defaul values, but I've explicitely written them out to show how you can edit them)\r\n",
        "decoder_type: rnn\r\n",
        "encoder_type: rnn \r\n",
        "enc_layers: 2\r\n",
        "dec_layers: 2\r\n",
        "enc_rnn_size: 500\r\n",
        "dec_rnn_size: 500\r\n",
        "dropout: 0.3\r\n",
        "global_attention : dot\r\n",
        "\r\n",
        "\r\n",
        "# Optimizer settings\r\n",
        "optim: sgd\r\n",
        "learning_rate: 1\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "Here the config file covers two major things: Model checkpointing and Model Hyperparameters.\r\n",
        "\r\n",
        "Certain settings are available only for certain models, for instance you wouldn't (want to) use positional encoding for an RNN-based model, however, it is necessary for proper training of Transformers and we could include it if we added a line ```positional_encoding: 'true'```.\r\n",
        "\r\n",
        "If we wanted to know more about any of these settings, we could take a peek at the OpenNMT [train documentation](https://opennmt.net/OpenNMT-py/options/train.html)\r\n",
        "\r\n",
        "For instance for the encoder options, it shows what available models can be used:\r\n",
        "```\r\n",
        "--encoder_type, -encoder_type\r\n",
        "Possible choices: rnn, brnn, ggnn, mean, transformer, cnn, transformer_lm\r\n",
        "\r\n",
        "Type of encoder layer to use. Non-RNN layers are experimental. Options are [rnn|brnn|ggnn|mean|transformer|cnn|transformer_lm].\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "Finally we will train our model with this configuration. (It took about 10 minutes for the small RNN model to train). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJmdN6G_qnjo",
        "outputId": "0ac39bd6-66e4-45c2-ddad-eed1a88431bb"
      },
      "source": [
        "!onmt_train -config toy-ende.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-15 22:37:41,310 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-03-15 22:37:41,310 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-03-15 22:37:41,311 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-03-15 22:37:41,311 INFO] Parsed 2 corpora from -data.\n",
            "[2021-03-15 22:37:41,311 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-03-15 22:37:41,311 INFO] Loading vocab from text file...\n",
            "[2021-03-15 22:37:41,311 INFO] Loading src vocabulary from toy-ende/run/example.vocab.src\n",
            "[2021-03-15 22:37:41,353 INFO] Loaded src vocab has 24995 tokens.\n",
            "[2021-03-15 22:37:41,362 INFO] Loading tgt vocabulary from toy-ende/run/example.vocab.tgt\n",
            "[2021-03-15 22:37:41,434 INFO] Loaded tgt vocab has 35816 tokens.\n",
            "[2021-03-15 22:37:41,447 INFO] Building fields with vocab in counters...\n",
            "[2021-03-15 22:37:41,511 INFO]  * tgt vocab size: 35820.\n",
            "[2021-03-15 22:37:41,540 INFO]  * src vocab size: 24997.\n",
            "[2021-03-15 22:37:41,542 INFO]  * src vocab size = 24997\n",
            "[2021-03-15 22:37:41,542 INFO]  * tgt vocab size = 35820\n",
            "[2021-03-15 22:37:41,546 INFO] Building model...\n",
            "[2021-03-15 22:37:45,149 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(24997, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(35820, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=35820, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-03-15 22:37:45,149 INFO] encoder: 16506500\n",
            "[2021-03-15 22:37:45,149 INFO] decoder: 41363820\n",
            "[2021-03-15 22:37:45,149 INFO] * number of parameters: 57870320\n",
            "[2021-03-15 22:37:45,150 INFO] Starting training on GPU: [0]\n",
            "[2021-03-15 22:37:45,150 INFO] Start training loop and validate every 500 steps...\n",
            "[2021-03-15 22:37:45,151 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-03-15 22:37:45,151 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:38:01,640 INFO] Step 100/10000; acc:   3.91; ppl: 89034.39; xent: 11.40; lr: 1.00000; 8745/8691 tok/s;     16 sec\n",
            "[2021-03-15 22:38:06,171 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:38:18,096 INFO] Step 200/10000; acc:   4.98; ppl: 9778.75; xent: 9.19; lr: 1.00000; 8674/8653 tok/s;     33 sec\n",
            "[2021-03-15 22:38:32,972 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:38:34,997 INFO] Step 300/10000; acc:   9.37; ppl: 2038.43; xent: 7.62; lr: 1.00000; 8567/8520 tok/s;     50 sec\n",
            "[2021-03-15 22:38:52,128 INFO] Step 400/10000; acc:  10.21; ppl: 1470.47; xent: 7.29; lr: 1.00000; 8508/8421 tok/s;     67 sec\n",
            "[2021-03-15 22:39:00,598 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:39:09,785 INFO] Step 500/10000; acc:  11.59; ppl: 1139.41; xent: 7.04; lr: 1.00000; 8121/8118 tok/s;     85 sec\n",
            "[2021-03-15 22:39:09,785 INFO] valid's transforms: TransformPipe()\n",
            "[2021-03-15 22:39:09,785 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
            "[2021-03-15 22:39:15,343 INFO] Validation perplexity: 1290.59\n",
            "[2021-03-15 22:39:15,343 INFO] Validation accuracy: 11.1671\n",
            "[2021-03-15 22:39:15,343 INFO] Model is improving ppl: inf --> 1290.59.\n",
            "[2021-03-15 22:39:15,343 INFO] Model is improving acc: -inf --> 11.1671.\n",
            "[2021-03-15 22:39:15,541 INFO] Saving checkpoint toy-ende/run/model_step_500.pt\n",
            "[2021-03-15 22:39:33,911 INFO] Step 600/10000; acc:  12.80; ppl: 926.92; xent: 6.83; lr: 1.00000; 5939/5918 tok/s;    109 sec\n",
            "[2021-03-15 22:39:35,151 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:39:51,127 INFO] Step 700/10000; acc:  13.39; ppl: 797.23; xent: 6.68; lr: 1.00000; 8558/8462 tok/s;    126 sec\n",
            "[2021-03-15 22:40:02,654 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:40:08,041 INFO] Step 800/10000; acc:  14.39; ppl: 675.96; xent: 6.52; lr: 1.00000; 8340/8358 tok/s;    143 sec\n",
            "[2021-03-15 22:40:25,561 INFO] Step 900/10000; acc:  14.87; ppl: 607.13; xent: 6.41; lr: 1.00000; 8201/8157 tok/s;    160 sec\n",
            "[2021-03-15 22:40:30,396 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:40:42,564 INFO] Step 1000/10000; acc:  15.47; ppl: 536.69; xent: 6.29; lr: 1.00000; 8440/8377 tok/s;    177 sec\n",
            "[2021-03-15 22:40:42,564 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
            "[2021-03-15 22:40:48,146 INFO] Validation perplexity: 961.292\n",
            "[2021-03-15 22:40:48,146 INFO] Validation accuracy: 14.0265\n",
            "[2021-03-15 22:40:48,146 INFO] Model is improving ppl: 1290.59 --> 961.292.\n",
            "[2021-03-15 22:40:48,146 INFO] Model is improving acc: 11.1671 --> 14.0265.\n",
            "[2021-03-15 22:40:48,340 INFO] Saving checkpoint toy-ende/run/model_step_1000.pt\n",
            "[2021-03-15 22:41:04,833 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
            "[2021-03-15 22:41:06,862 INFO] Step 1100/10000; acc:  15.89; ppl: 480.34; xent: 6.17; lr: 1.00000; 5939/5932 tok/s;    202 sec\n",
            "[2021-03-15 22:41:24,197 INFO] Step 1200/10000; acc:  16.60; ppl: 421.22; xent: 6.04; lr: 1.00000; 8424/8349 tok/s;    219 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/OpenNMT-py/onmt/trainer.py\", line 368, in _gradient_accumulation\n",
            "    with_align=self.with_align)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/OpenNMT-py/onmt/models/model.py\", line 69, in forward\n",
            "    with_align=with_align)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/OpenNMT-py/onmt/decoders/decoder.py\", line 214, in forward\n",
            "    tgt, memory_bank, memory_lengths=memory_lengths)\n",
            "  File \"/content/OpenNMT-py/onmt/decoders/decoder.py\", line 391, in _run_forward_pass\n",
            "    rnn_output, dec_state = self.rnn(decoder_input, dec_state)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/OpenNMT-py/onmt/models/stacked_rnn.py\", line 26, in forward\n",
            "    h_1_i, c_1_i = layer(input_feed, (h_0[i], c_0[i]))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\", line 969, in forward\n",
            "    self.bias_ih, self.bias_hh,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('OpenNMT-py', 'console_scripts', 'onmt_train')())\n",
            "  File \"/content/OpenNMT-py/onmt/bin/train.py\", line 169, in main\n",
            "    train(opt)\n",
            "  File \"/content/OpenNMT-py/onmt/bin/train.py\", line 154, in train\n",
            "    train_process(opt, device_id=0)\n",
            "  File \"/content/OpenNMT-py/onmt/train_single.py\", line 112, in main\n",
            "    valid_steps=opt.valid_steps)\n",
            "  File \"/content/OpenNMT-py/onmt/trainer.py\", line 244, in train\n",
            "    report_stats)\n",
            "  File \"/content/OpenNMT-py/onmt/trainer.py\", line 368, in _gradient_accumulation\n",
            "    with_align=self.with_align)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYpT9fpCziZJ"
      },
      "source": [
        "Once our model is saved. We can use it to actually generate predictions on our output files. Our models will be saved under the ```save_model``` setting of our config file, in this case: ```toy-ende/run/model_```  Since we are only saving every 500 training steps, and keeping the past three checkpoints, we can choose from the available models. ```model_step_1000.pt``` and ```model_step_1500.pt``` and ```model_step_2000.pt```. Our early stopping indicates the best model (lowest perplexity/highest acc) of the three is 1000, but let's look at how to pick between these three using BLEU:\r\n",
        "\r\n",
        "## Translating\r\n",
        "\r\n",
        "To do so we will need to translate the source sentences, decoding with Beam search, in this case we've chosen a ```-beam_size``` of 10, however you will be asked in the question to adjust it to different sizes.\r\n",
        "\r\n",
        "Let's first create predictions for our ```_step_2000.pt```, ```_step_2500.pt``` , ```_step_3000.pt``` models (NOTE YOUR MODEL MAY HAVE STOPPED AT A DIFFERENT POINT, IN WHICH CASE USE THE APPROPRIATE 3 LAST CHECKPOINTS):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myhsvgu34DAj",
        "outputId": "06550685-f851-4dd4-e6de-657ba441eca7"
      },
      "source": [
        "!onmt_translate -model toy-ende/run/model_step_2000.pt -src toy-ende/src-val.txt -output toy-ende/val_2000.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\r\n",
        "!onmt_translate -model toy-ende/run/model_step_2500.pt -src toy-ende/src-val.txt -output toy-ende/val_2500.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\r\n",
        "!onmt_translate -model toy-ende/run/model_step_3000.pt -src toy-ende/src-val.txt -output toy-ende/val_3000.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-02 00:55:11,643 INFO] Translating shard 0.\n",
            "[2021-03-02 00:57:56,340 INFO] PRED AVG SCORE: -1.8684, PRED PPL: 6.4777\n",
            "[2021-03-02 00:58:00,669 INFO] Translating shard 0.\n",
            "[2021-03-02 01:00:08,496 INFO] PRED AVG SCORE: -1.7845, PRED PPL: 5.9568\n",
            "[2021-03-02 01:00:12,873 INFO] Translating shard 0.\n",
            "[2021-03-02 01:02:52,549 INFO] PRED AVG SCORE: -1.6705, PRED PPL: 5.3148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt-Z2qmTAzQa"
      },
      "source": [
        "[Note we can now manually inspect the results under val_*.txt]\r\n",
        "\r\n",
        "Finally let's calculate the BLEU scores of the outputs! We would eventually want to select the model with Highest BLEU (in our case 37 with our 2500 step model) and use this on our test set.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7NMugxgA34T",
        "outputId": "18918afd-c51b-48ca-c2d0-69912c49b53a"
      },
      "source": [
        "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_2000.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_2500.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_3000.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use of uninitialized value in division (/) at OpenNMT-py/tools/multi-bleu.perl line 139, <STDIN> line 3000.\n",
            "BLEU = 0.00, 18.0/1.0/0.1/0.0 (BP=0.873, ratio=0.880, hyp_len=63094, ref_len=71666)\n",
            "BLEU = 0.37, 21.5/1.8/0.2/0.0 (BP=0.550, ratio=0.626, hyp_len=44858, ref_len=71666)\n",
            "BLEU = 0.32, 18.5/1.0/0.1/0.0 (BP=0.866, ratio=0.874, hyp_len=62645, ref_len=71666)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UylIL0MBg-NO"
      },
      "source": [
        "# Teamwork Exercise 3\r\n",
        "\r\n",
        "We have seen how OpenNMT can be used, now let's apply it to our Multi30k dataset.\r\n",
        "\r\n",
        "You can run your code in here and then download the results to submit on github.\r\n",
        "\r\n",
        "This is a Team assignment to enable students helping one another understand the different components of the OpenNMT framework and running them correctly.\r\n",
        "\r\n",
        "*You are provided with a Multi30k.yaml to fill in, be sure to submit this alongside your colab notebook and other files in the repository.*\r\n",
        "\r\n",
        "## T3.1\r\n",
        "\r\n",
        "### Build the vocab for the Multi30k En-Fr dataset\r\n",
        "\r\n",
        "While just having a vocabulary is fine for some cases, using a sub-word tokenization might help capture morphological information better.\r\n",
        "\r\n",
        "To do this, in your config file add ```transforms: [sentencepiece, filtertoolong]``` to both the training and validation corpora.\r\n",
        "\r\n",
        "Give the code you ran to build the vocab as well as the \"data\" section of your multi30k config file.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cczqnP8r06B",
        "outputId": "d9ccb766-30f2-49a1-eb0c-a3c13bdb093c"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\r\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.59.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.59.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp37-none-any.whl size=14727027 sha256=33865127c8a003e45bf039fde79d007bf0f9f9a6a7d3cfbdb6850095a94e6aa7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ahrpeoig/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KD6CBVjcQu"
      },
      "source": [
        "# TODO build Multi30k Vocab\r\n",
        "import fr_core_news_sm\r\n",
        "import en_core_web_sm\r\n",
        "spacy_fr = fr_core_news_sm.load()\r\n",
        "spacy_en = en_core_web_sm.load()\r\n",
        "\r\n",
        "for file in [\"train.fr\",\"val.fr\"]:\r\n",
        "  with open(file,\"r\", encoding='utf-8') as fp:\r\n",
        "    outfile = file + \".tokd\"\r\n",
        "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\r\n",
        "      for line in fp:\r\n",
        "        tokenized = [tok.text for tok in spacy_fr.tokenizer(line)]\r\n",
        "        out.write(\" \".join(tokenized))\r\n",
        "for file in [\"train-1.en\",\"val.en\"]:\r\n",
        "  with open(file,\"r\", encoding='utf-8') as fp:\r\n",
        "    outfile = file + \".tokd\"\r\n",
        "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\r\n",
        "      for line in fp:\r\n",
        "        tokenized = [tok.text for tok in spacy_en.tokenizer(line)]\r\n",
        "        out.write(\" \".join(tokenized))           \r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhCmpQgirlUY",
        "outputId": "eeb4a210-401a-450e-a3ed-e2a6c055ef85"
      },
      "source": [
        "!onmt_build_vocab -config multi30k_sol.yml -n_sample 10000\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-03-16 01:45:38,941 INFO] Counter vocab from 10000 samples.\n",
            "[2021-03-16 01:45:38,941 INFO] Build vocab on 10000 transformed examples/corpus.\n",
            "[2021-03-16 01:45:38,947 INFO] corpus_1's transforms: TransformPipe(FilterTooLongTransform(src_seq_length=200, tgt_seq_length=200))\n",
            "[2021-03-16 01:45:38,947 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:45:39,148 INFO] Counters src:6872\n",
            "[2021-03-16 01:45:39,148 INFO] Counters tgt:6412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsVLrM0Pjfn8"
      },
      "source": [
        "```\r\n",
        "Include changes you made to the Data saving, Corpus, and Vocab section in the Config HERE\r\n",
        "\r\n",
        "# multi30k.yaml\r\n",
        "\r\n",
        "## TO DO COMPLETE DATA SAVING\r\n",
        "save_data: multi30k/run/\r\n",
        "## Where the vocab(s) will be written\r\n",
        "src_vocab: multi30k/run/vocab.src\r\n",
        "tgt_vocab: multi30k/run/vocab.tgt\r\n",
        "\r\n",
        "# Corpus opts:\r\n",
        "data:\r\n",
        "## TODO COMPLETE CORPUS OPTIONS\r\n",
        "## Add sentencepiece and filter long segments\r\n",
        "    corpus_1:\r\n",
        "        path_src: multi30k/train.fr.tokd\r\n",
        "        path_tgt: multi30k/train-1.en.tokd\r\n",
        "        transforms: [filtertoolong]\r\n",
        "    valid:\r\n",
        "        path_src: multi30k/val.fr.tokd\r\n",
        "        path_tgt: multi30k/val.en.tokd\r\n",
        "        #NOTE NO FILTERTOOLONG --> You don't want to bias your validation score\r\n",
        "\r\n",
        "\r\n",
        "#TODO Fill in vocab you create (already have this above)\r\n",
        "src_vocab: multi30k/run/vocab.src\r\n",
        "tgt_vocab: multi30k/run/vocab.tgt\r\n",
        "\r\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B05G-PaWl0L-"
      },
      "source": [
        "## T3.2\r\n",
        "Train Model\r\n",
        "\r\n",
        "Fill in the multi30k.yaml config to setup a seq2seq model that has a 3 layer RNN encoder 2 layer RNN decoder, MLP attention, with 20% dropout, using Adam as your optimizer.\r\n",
        "\r\n",
        "Copy and paste the changed parts of the *.yml file below along with the training command you used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcx3Uuamdil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e1e8cf-1a14-4ba8-d2e6-d9fe8ee332a6"
      },
      "source": [
        "# TODO Train Model\r\n",
        "\r\n",
        "!onmt_train -config multi30k_sol.yml"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:47:00,107 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-03-16 01:47:00,107 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-03-16 01:47:00,107 INFO] Parsed 2 corpora from -data.\n",
            "[2021-03-16 01:47:00,108 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-03-16 01:47:00,108 INFO] Loading vocab from text file...\n",
            "[2021-03-16 01:47:00,108 INFO] Loading src vocabulary from multi30k/run/vocab.src\n",
            "[2021-03-16 01:47:00,121 INFO] Loaded src vocab has 6872 tokens.\n",
            "[2021-03-16 01:47:00,123 INFO] Loading tgt vocabulary from multi30k/run/vocab.tgt\n",
            "[2021-03-16 01:47:00,134 INFO] Loaded tgt vocab has 6412 tokens.\n",
            "[2021-03-16 01:47:00,136 INFO] Building fields with vocab in counters...\n",
            "[2021-03-16 01:47:00,143 INFO]  * tgt vocab size: 6416.\n",
            "[2021-03-16 01:47:00,151 INFO]  * src vocab size: 6874.\n",
            "[2021-03-16 01:47:00,151 INFO]  * src vocab size = 6874\n",
            "[2021-03-16 01:47:00,151 INFO]  * tgt vocab size = 6416\n",
            "[2021-03-16 01:47:00,154 INFO] Building model...\n",
            "[2021-03-16 01:47:03,252 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6874, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 500, num_layers=3, dropout=0.2)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6416, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_context): Linear(in_features=500, out_features=500, bias=False)\n",
            "      (linear_query): Linear(in_features=500, out_features=500, bias=True)\n",
            "      (v): Linear(in_features=500, out_features=1, bias=False)\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=6416, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-03-16 01:47:03,252 INFO] encoder: 9449000\n",
            "[2021-03-16 01:47:03,252 INFO] decoder: 12431916\n",
            "[2021-03-16 01:47:03,252 INFO] * number of parameters: 21880916\n",
            "[2021-03-16 01:47:03,253 INFO] Starting training on GPU: [0]\n",
            "[2021-03-16 01:47:03,253 INFO] Start training loop and validate every 500 steps...\n",
            "[2021-03-16 01:47:03,253 INFO] corpus_1's transforms: TransformPipe(FilterTooLongTransform(src_seq_length=200, tgt_seq_length=200))\n",
            "[2021-03-16 01:47:03,254 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:47:09,820 INFO] Step 100/10000; acc:  21.93; ppl: 174.48; xent: 5.16; lr: 0.00100; 13752/13472 tok/s;      7 sec\n",
            "[2021-03-16 01:47:16,022 INFO] Step 200/10000; acc:  35.89; ppl: 52.10; xent: 3.95; lr: 0.00100; 14101/13949 tok/s;     13 sec\n",
            "[2021-03-16 01:47:22,592 INFO] Step 300/10000; acc:  40.08; ppl: 32.84; xent: 3.49; lr: 0.00100; 13895/13687 tok/s;     19 sec\n",
            "[2021-03-16 01:47:29,345 INFO] Step 400/10000; acc:  41.75; ppl: 27.98; xent: 3.33; lr: 0.00100; 13957/13850 tok/s;     26 sec\n",
            "[2021-03-16 01:47:32,686 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:47:36,028 INFO] Step 500/10000; acc:  43.06; ppl: 28.01; xent: 3.33; lr: 0.00100; 14170/13879 tok/s;     33 sec\n",
            "[2021-03-16 01:47:36,029 INFO] valid's transforms: TransformPipe()\n",
            "[2021-03-16 01:47:36,029 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:47:36,867 INFO] Validation perplexity: 21.8674\n",
            "[2021-03-16 01:47:36,867 INFO] Validation accuracy: 45.8934\n",
            "[2021-03-16 01:47:36,867 INFO] Model is improving ppl: inf --> 21.8674.\n",
            "[2021-03-16 01:47:36,867 INFO] Model is improving acc: -inf --> 45.8934.\n",
            "[2021-03-16 01:47:36,917 INFO] Saving checkpoint multi30k/run/model_step_500.pt\n",
            "[2021-03-16 01:47:44,109 INFO] Step 600/10000; acc:  47.63; ppl: 20.99; xent: 3.04; lr: 0.00100; 11009/10851 tok/s;     41 sec\n",
            "[2021-03-16 01:47:50,668 INFO] Step 700/10000; acc:  49.63; ppl: 15.93; xent: 2.77; lr: 0.00100; 13552/13374 tok/s;     47 sec\n",
            "[2021-03-16 01:47:57,309 INFO] Step 800/10000; acc:  50.94; ppl: 14.43; xent: 2.67; lr: 0.00100; 13809/13689 tok/s;     54 sec\n",
            "[2021-03-16 01:48:03,994 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:48:04,262 INFO] Step 900/10000; acc:  50.18; ppl: 15.37; xent: 2.73; lr: 0.00100; 13781/13646 tok/s;     61 sec\n",
            "[2021-03-16 01:48:10,805 INFO] Step 1000/10000; acc:  54.90; ppl: 12.03; xent: 2.49; lr: 0.00100; 13872/13592 tok/s;     68 sec\n",
            "[2021-03-16 01:48:10,806 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:48:11,722 INFO] Validation perplexity: 11.574\n",
            "[2021-03-16 01:48:11,722 INFO] Validation accuracy: 56.3435\n",
            "[2021-03-16 01:48:11,722 INFO] Model is improving ppl: 21.8674 --> 11.574.\n",
            "[2021-03-16 01:48:11,722 INFO] Model is improving acc: 45.8934 --> 56.3435.\n",
            "[2021-03-16 01:48:11,777 INFO] Saving checkpoint multi30k/run/model_step_1000.pt\n",
            "[2021-03-16 01:48:19,165 INFO] Step 1100/10000; acc:  59.57; ppl:  8.74; xent: 2.17; lr: 0.00100; 10524/10394 tok/s;     76 sec\n",
            "[2021-03-16 01:48:25,897 INFO] Step 1200/10000; acc:  62.55; ppl:  6.93; xent: 1.94; lr: 0.00100; 13546/13345 tok/s;     83 sec\n",
            "[2021-03-16 01:48:32,670 INFO] Step 1300/10000; acc:  64.54; ppl:  6.24; xent: 1.83; lr: 0.00100; 13793/13673 tok/s;     89 sec\n",
            "[2021-03-16 01:48:35,927 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:48:39,648 INFO] Step 1400/10000; acc:  64.16; ppl:  6.50; xent: 1.87; lr: 0.00100; 13536/13357 tok/s;     96 sec\n",
            "[2021-03-16 01:48:46,242 INFO] Step 1500/10000; acc:  69.40; ppl:  4.76; xent: 1.56; lr: 0.00100; 13823/13498 tok/s;    103 sec\n",
            "[2021-03-16 01:48:46,242 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:48:47,163 INFO] Validation perplexity: 5.92613\n",
            "[2021-03-16 01:48:47,164 INFO] Validation accuracy: 67.7078\n",
            "[2021-03-16 01:48:47,164 INFO] Model is improving ppl: 11.574 --> 5.92613.\n",
            "[2021-03-16 01:48:47,164 INFO] Model is improving acc: 56.3435 --> 67.7078.\n",
            "[2021-03-16 01:48:47,215 INFO] Saving checkpoint multi30k/run/model_step_1500.pt\n",
            "[2021-03-16 01:48:54,384 INFO] Step 1600/10000; acc:  71.43; ppl:  3.99; xent: 1.38; lr: 0.00100; 10582/10493 tok/s;    111 sec\n",
            "[2021-03-16 01:49:01,166 INFO] Step 1700/10000; acc:  71.52; ppl:  3.95; xent: 1.37; lr: 0.00100; 13614/13494 tok/s;    118 sec\n",
            "[2021-03-16 01:49:07,837 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:49:08,361 INFO] Step 1800/10000; acc:  69.71; ppl:  4.35; xent: 1.47; lr: 0.00100; 13283/13147 tok/s;    125 sec\n",
            "[2021-03-16 01:49:15,139 INFO] Step 1900/10000; acc:  72.64; ppl:  3.64; xent: 1.29; lr: 0.00100; 13530/13232 tok/s;    132 sec\n",
            "[2021-03-16 01:49:21,776 INFO] Step 2000/10000; acc:  75.07; ppl:  3.10; xent: 1.13; lr: 0.00100; 13349/13170 tok/s;    139 sec\n",
            "[2021-03-16 01:49:21,776 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:49:22,594 INFO] Validation perplexity: 4.38414\n",
            "[2021-03-16 01:49:22,594 INFO] Validation accuracy: 71.5997\n",
            "[2021-03-16 01:49:22,594 INFO] Model is improving ppl: 5.92613 --> 4.38414.\n",
            "[2021-03-16 01:49:22,594 INFO] Model is improving acc: 67.7078 --> 71.5997.\n",
            "[2021-03-16 01:49:22,645 INFO] Saving checkpoint multi30k/run/model_step_2000.pt\n",
            "[2021-03-16 01:49:29,989 INFO] Step 2100/10000; acc:  75.51; ppl:  3.03; xent: 1.11; lr: 0.00100; 10965/10845 tok/s;    147 sec\n",
            "[2021-03-16 01:49:36,851 INFO] Step 2200/10000; acc:  74.83; ppl:  3.14; xent: 1.14; lr: 0.00100; 13548/13424 tok/s;    154 sec\n",
            "[2021-03-16 01:49:39,874 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:49:44,117 INFO] Step 2300/10000; acc:  73.23; ppl:  3.38; xent: 1.22; lr: 0.00100; 13334/13074 tok/s;    161 sec\n",
            "[2021-03-16 01:49:50,795 INFO] Step 2400/10000; acc:  78.46; ppl:  2.49; xent: 0.91; lr: 0.00100; 13293/13094 tok/s;    168 sec\n",
            "[2021-03-16 01:49:57,422 INFO] Step 2500/10000; acc:  78.37; ppl:  2.49; xent: 0.91; lr: 0.00100; 13076/12943 tok/s;    174 sec\n",
            "[2021-03-16 01:49:57,423 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:49:58,343 INFO] Validation perplexity: 4.32727\n",
            "[2021-03-16 01:49:58,343 INFO] Validation accuracy: 72.2507\n",
            "[2021-03-16 01:49:58,343 INFO] Model is improving ppl: 4.38414 --> 4.32727.\n",
            "[2021-03-16 01:49:58,343 INFO] Model is improving acc: 71.5997 --> 72.2507.\n",
            "[2021-03-16 01:49:58,395 INFO] Saving checkpoint multi30k/run/model_step_2500.pt\n",
            "[2021-03-16 01:50:06,001 INFO] Step 2600/10000; acc:  77.51; ppl:  2.62; xent: 0.96; lr: 0.00100; 10765/10645 tok/s;    183 sec\n",
            "[2021-03-16 01:50:12,334 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:50:13,173 INFO] Step 2700/10000; acc:  76.14; ppl:  2.84; xent: 1.04; lr: 0.00100; 13359/13221 tok/s;    190 sec\n",
            "[2021-03-16 01:50:20,090 INFO] Step 2800/10000; acc:  78.90; ppl:  2.40; xent: 0.88; lr: 0.00100; 13386/13087 tok/s;    197 sec\n",
            "[2021-03-16 01:50:26,604 INFO] Step 2900/10000; acc:  81.29; ppl:  2.11; xent: 0.75; lr: 0.00100; 13499/13346 tok/s;    203 sec\n",
            "[2021-03-16 01:50:33,315 INFO] Step 3000/10000; acc:  80.25; ppl:  2.22; xent: 0.80; lr: 0.00100; 13326/13191 tok/s;    210 sec\n",
            "[2021-03-16 01:50:33,315 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:50:34,148 INFO] Validation perplexity: 4.29523\n",
            "[2021-03-16 01:50:34,148 INFO] Validation accuracy: 72.5208\n",
            "[2021-03-16 01:50:34,148 INFO] Model is improving ppl: 4.32727 --> 4.29523.\n",
            "[2021-03-16 01:50:34,149 INFO] Model is improving acc: 72.2507 --> 72.5208.\n",
            "[2021-03-16 01:50:34,199 INFO] Saving checkpoint multi30k/run/model_step_3000.pt\n",
            "[2021-03-16 01:50:42,009 INFO] Step 3100/10000; acc:  79.05; ppl:  2.37; xent: 0.86; lr: 0.00100; 10850/10717 tok/s;    219 sec\n",
            "[2021-03-16 01:50:46,988 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:50:49,143 INFO] Step 3200/10000; acc:  78.32; ppl:  2.44; xent: 0.89; lr: 0.00100; 13282/13076 tok/s;    226 sec\n",
            "[2021-03-16 01:50:55,841 INFO] Step 3300/10000; acc:  83.53; ppl:  1.89; xent: 0.63; lr: 0.00100; 13362/13148 tok/s;    233 sec\n",
            "[2021-03-16 01:51:02,393 INFO] Step 3400/10000; acc:  82.61; ppl:  1.95; xent: 0.67; lr: 0.00100; 13295/13176 tok/s;    239 sec\n",
            "[2021-03-16 01:51:09,335 INFO] Step 3500/10000; acc:  81.63; ppl:  2.05; xent: 0.72; lr: 0.00100; 13348/13150 tok/s;    246 sec\n",
            "[2021-03-16 01:51:09,335 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:51:10,158 INFO] Validation perplexity: 4.36675\n",
            "[2021-03-16 01:51:10,158 INFO] Validation accuracy: 73.1579\n",
            "[2021-03-16 01:51:10,158 INFO] Stalled patience: 1/2\n",
            "[2021-03-16 01:51:10,209 INFO] Saving checkpoint multi30k/run/model_step_3500.pt\n",
            "[2021-03-16 01:51:18,349 INFO] Step 3600/10000; acc:  80.19; ppl:  2.21; xent: 0.79; lr: 0.00100; 10636/10544 tok/s;    255 sec\n",
            "[2021-03-16 01:51:19,527 INFO] Loading ParallelCorpus(multi30k/train.fr.tokd, multi30k/train-1.en.tokd, align=None)...\n",
            "[2021-03-16 01:51:25,321 INFO] Step 3700/10000; acc:  82.99; ppl:  1.92; xent: 0.65; lr: 0.00100; 13234/12927 tok/s;    262 sec\n",
            "[2021-03-16 01:51:31,908 INFO] Step 3800/10000; acc:  85.36; ppl:  1.71; xent: 0.54; lr: 0.00100; 13345/13189 tok/s;    269 sec\n",
            "[2021-03-16 01:51:38,827 INFO] Step 3900/10000; acc:  83.52; ppl:  1.86; xent: 0.62; lr: 0.00100; 13125/12921 tok/s;    276 sec\n",
            "[2021-03-16 01:51:45,718 INFO] Step 4000/10000; acc:  83.51; ppl:  1.86; xent: 0.62; lr: 0.00100; 13276/13211 tok/s;    282 sec\n",
            "[2021-03-16 01:51:45,719 INFO] Loading ParallelCorpus(multi30k/val.fr.tokd, multi30k/val.en.tokd, align=None)...\n",
            "[2021-03-16 01:51:46,628 INFO] Validation perplexity: 4.54937\n",
            "[2021-03-16 01:51:46,628 INFO] Validation accuracy: 72.8116\n",
            "[2021-03-16 01:51:46,628 INFO] Stalled patience: 0/2\n",
            "[2021-03-16 01:51:46,628 INFO] Training finished after stalled validations. Early Stop!\n",
            "[2021-03-16 01:51:46,628 INFO] Best model found at step 3000\n",
            "[2021-03-16 01:51:46,680 INFO] Saving checkpoint multi30k/run/model_step_4000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBlHa2Wlm4Q9"
      },
      "source": [
        "```\r\n",
        "Changes to model, and optimizer here.\r\n",
        "\r\n",
        "#Changes to model, and optimizer here.\r\n",
        "decoder_type: rnn\r\n",
        "encoder_type: rnn \r\n",
        "enc_layers: 3\r\n",
        "dec_layers: 2\r\n",
        "enc_rnn_size: 500\r\n",
        "dec_rnn_size: 500\r\n",
        "dropout: 0.2\r\n",
        "global_attention : mlp\r\n",
        "\r\n",
        "\r\n",
        "# Optimizer settings\r\n",
        "optim: adam\r\n",
        "learning_rate: 0.001  #note changing Adam you should change the starting learning rate\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6WGJkJym-y9"
      },
      "source": [
        "## T3.3\r\n",
        "\r\n",
        "Decoding\r\n",
        "\r\n",
        "Create predictions for the validation set using your saved models and select the one that has the highest BLEU. You should set beam size to 5 for each of these models.\r\n",
        "\r\n",
        "Report the BLEU on this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3n0ezhfm9q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6592f7-4143-4660-a09c-1e3ec6a643e9"
      },
      "source": [
        "## Code to create predictions and calculate BLEU for models\r\n",
        "\r\n",
        "!onmt_translate -model multi30k/run/model_step_3000.pt -src multi30k/val.fr.tokd -output multi30k/val_3000.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/val_3500.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n",
        "!onmt_translate -model multi30k/run/model_step_4000.pt -src multi30k/val.fr.tokd -output multi30k/val_4000.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n",
        "\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:53:41,325 INFO] Translating shard 0.\n",
            "[2021-03-16 01:53:54,390 INFO] PRED AVG SCORE: -0.3489, PRED PPL: 1.4175\n",
            "[2021-03-16 01:53:58,297 INFO] Translating shard 0.\n",
            "[2021-03-16 01:54:11,366 INFO] PRED AVG SCORE: -0.3111, PRED PPL: 1.3649\n",
            "[2021-03-16 01:54:15,266 INFO] Translating shard 0.\n",
            "[2021-03-16 01:54:27,576 INFO] PRED AVG SCORE: -0.2963, PRED PPL: 1.3449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1WsYxibualR",
        "outputId": "cd0be25f-c274-4f16-8812-a0691bc59c56"
      },
      "source": [
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/val_3000.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/val_3500.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/val_4000.txt\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU = 43.32, 72.5/51.1/37.4/27.4 (BP=0.981, ratio=0.981, hyp_len=13172, ref_len=13426)\n",
            "BLEU = 43.85, 73.1/51.7/38.0/28.1 (BP=0.978, ratio=0.978, hyp_len=13133, ref_len=13426)\n",
            "BLEU = 43.09, 73.3/51.8/37.9/27.9 (BP=0.963, ratio=0.963, hyp_len=12933, ref_len=13426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcF9qBqknZg0"
      },
      "source": [
        "## T3.4 \r\n",
        "\r\n",
        "Comparing Beam Width\r\n",
        "\r\n",
        "For your BEST model compare the peformance (Both BLEU and clocktime to run)  with the following Beam Sizes: 5 (done above), 10, 15, and 20.\r\n",
        "\r\n",
        "Give your code and outputs below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2slf_eRSvyWQ",
        "outputId": "7723fd79-ce1c-4f94-e4c4-42bacca074c2"
      },
      "source": [
        "%%time\r\n",
        "#INCLUDED FOR CURIOSITY SAKE\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam1.txt -gpu 0 -beam_size 1 -seed 531 -block_ngram 2\r\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:58:49,161 INFO] Translating shard 0.\n",
            "[2021-03-16 01:58:53,656 INFO] PRED AVG SCORE: -0.7389, PRED PPL: 2.0936\n",
            "CPU times: user 18.6 ms, sys: 12.2 ms, total: 30.7 ms\n",
            "Wall time: 8.26 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59Py87hwM0p",
        "outputId": "15c890ce-0d8e-4ab1-de73-562ea0e72b26"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam5.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:59:35,899 INFO] Translating shard 0.\n",
            "[2021-03-16 01:59:48,986 INFO] PRED AVG SCORE: -0.3111, PRED PPL: 1.3649\n",
            "CPU times: user 35.7 ms, sys: 12.2 ms, total: 47.9 ms\n",
            "Wall time: 17 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkYdBODGoJIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc17c6b6-62da-4096-9d74-5fb870715dce"
      },
      "source": [
        "%%time\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam10.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:56:26,498 INFO] Translating shard 0.\n",
            "[2021-03-16 01:56:49,723 INFO] PRED AVG SCORE: -0.2985, PRED PPL: 1.3478\n",
            "CPU times: user 49.2 ms, sys: 17.4 ms, total: 66.6 ms\n",
            "Wall time: 27.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ5mXi3gvtww",
        "outputId": "5ebc92a3-af50-411e-857b-0412f8b61336"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam15.txt -gpu 0 -beam_size 15 -seed 531 -block_ngram 2\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:57:22,825 INFO] Translating shard 0.\n",
            "[2021-03-16 01:57:57,147 INFO] PRED AVG SCORE: -0.2971, PRED PPL: 1.3459\n",
            "CPU times: user 67.3 ms, sys: 22.3 ms, total: 89.5 ms\n",
            "Wall time: 38.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWW51HOavt7X",
        "outputId": "a3b5ff3d-3bb2-4edc-ba42-7419c15b8765"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src multi30k/val.fr.tokd -output multi30k/beam20.txt -gpu 0 -beam_size 20 -seed 531 -block_ngram 2\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 01:58:01,617 INFO] Translating shard 0.\n",
            "[2021-03-16 01:58:45,384 INFO] PRED AVG SCORE: -0.2973, PRED PPL: 1.3462\n",
            "CPU times: user 84.8 ms, sys: 19 ms, total: 104 ms\n",
            "Wall time: 47.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ6jyc21wU1A",
        "outputId": "908fd994-ecac-4aaa-ac62-e8b4ef1044ef"
      },
      "source": [
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/beam1.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/beam5.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/beam10.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/beam15.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl multi30k/val.en.tokd < multi30k/beam20.txt\r\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU = 42.09, 69.3/48.7/35.5/26.2 (BP=1.000, ratio=1.035, hyp_len=13891, ref_len=13426)\n",
            "BLEU = 43.85, 73.1/51.7/38.0/28.1 (BP=0.978, ratio=0.978, hyp_len=13133, ref_len=13426)\n",
            "BLEU = 43.72, 73.3/52.1/38.2/28.2 (BP=0.971, ratio=0.971, hyp_len=13036, ref_len=13426)\n",
            "BLEU = 43.82, 73.5/52.2/38.4/28.5 (BP=0.968, ratio=0.968, hyp_len=13000, ref_len=13426)\n",
            "BLEU = 43.84, 73.7/52.4/38.5/28.5 (BP=0.967, ratio=0.967, hyp_len=12988, ref_len=13426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GQcdrkw3VJ"
      },
      "source": [
        "A Narrow Beam (5) is better than \"best path\" (beam=1) as well as larger beam sizes (10+)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJEDhpNrxPtK"
      },
      "source": [
        "import csv\r\n",
        "for file in [\"long_test_eng_fre.tsv\",\"short_test_eng_fre.tsv\"]:\r\n",
        "  with open(file,\"r\", encoding='utf-8') as tsv:\r\n",
        "    tsv_reader = csv.reader(tsv, delimiter =\"\\t\")\r\n",
        "    next(tsv_reader, None) \r\n",
        "    outfile_fr = file + \".fr.tokd\"\r\n",
        "    outfile_en = file + \".en.tokd\"\r\n",
        "    with open(outfile_fr, \"w\", encoding=\"utf-8\") as out_fr:\r\n",
        "      with open(outfile_en, \"w\", encoding=\"utf-8\") as out_en:\r\n",
        "        for row in tsv_reader:\r\n",
        "          tokenized_en = [tok.text for tok in spacy_en.tokenizer(row[0])]\r\n",
        "          tokenized_fr = [tok.text for tok in spacy_fr.tokenizer(row[1])]\r\n",
        "          out_fr.write(\" \".join(tokenized_fr)+\"\\n\")\r\n",
        "          out_en.write(\" \".join(tokenized_en)+\"\\n\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyC1MwUvxHE4",
        "outputId": "9ea85f45-7c57-4d99-a0ed-5485fd8f580f"
      },
      "source": [
        "#Part 2 \r\n",
        "\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src long_test_eng_fre.tsv.fr.tokd -output long_trans.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n",
        "!onmt_translate -model multi30k/run/model_step_3500.pt -src short_test_eng_fre.tsv.fr.tokd -output short_trans.txt -gpu 0 -beam_size 5 -seed 531 -block_ngram 2\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-16 02:05:11,086 INFO] Translating shard 0.\n",
            "[2021-03-16 02:05:12,671 INFO] PRED AVG SCORE: -0.4366, PRED PPL: 1.5474\n",
            "[2021-03-16 02:05:16,495 INFO] Translating shard 0.\n",
            "[2021-03-16 02:05:17,091 INFO] PRED AVG SCORE: -0.2847, PRED PPL: 1.3294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H47MXoTCx56x",
        "outputId": "ee3ab8a7-e240-454d-ef75-5b84705cd155"
      },
      "source": [
        "!perl  OpenNMT-py/tools/multi-bleu.perl long_test_eng_fre.tsv.en.tokd < long_trans.txt\r\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl short_test_eng_fre.tsv.en.tokd < short_trans.txt\r\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU = 36.21, 69.1/45.2/31.7/23.0 (BP=0.933, ratio=0.935, hyp_len=1461, ref_len=1563)\n",
            "BLEU = 44.60, 70.7/51.3/38.2/28.5 (BP=1.000, ratio=1.055, hyp_len=672, ref_len=637)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
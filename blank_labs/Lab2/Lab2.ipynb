{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- seq2seq Models\n",
    "- Grid Search vs. Random Search for Hyperparameters Tuning\n",
    "\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.7.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 6, 2021, 23:59:00 (Vancouver time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 531\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the questions of this lab, we continue to use the English-French bilingual corpus of [Multi30k](https://github.com/multi30k/dataset) dataset. Our task is to `translate text from French language to English language`. All your model should be trained on `train_eng_fre.tsv`, validated on `val_eng_fre.tsv`, and tested on `test_eng_fre.tsv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise T2: Seq2Seq Tutorial Recap\n",
    "\n",
    "#### This exercise isn't meant to be too challenging, but we do want you to be able to talk about the code/architecture with your team in case you need help navigating the code.\n",
    "\n",
    "In the tutorial, we used a **uni-directional LSTM Encoder** to compress the information of a source language into two context representation vectors of a fixed length (i.e., the final hidden and cell states). Then, we use the final hidden and cell states to initialize the hidden and cell states of uni-directional LSTM Decoder. However, the capability of these representations can be limited. They can easily forget the earlier information of a long sequence and co-reference relationships. Before introducing the attention mechanism, we can also use some tricks to solve the issue of context **information bottleneck** problem. \n",
    "\n",
    "In this exercise, please write a script to implement the following tricks in **a single seq2seq model**:\n",
    "1. Use a **bi-directional LSTM** as the **Encoder** to get the context representation of the source sentence. \n",
    "\n",
    "2. Instead of using the final hidden and cell states of the bi-directional encoder to initialize the uni-directional decoder, (A) for $h_0$ use **the mean of all hidden states** and (B) for $c_0$ use the **final cell state** of the bi-directional encoder. \n",
    "\n",
    "Combine 1 & 2 in **a single seq2seq model**.\n",
    "\n",
    "**Instruction:**\n",
    "- Please paste your experiment codes to answer the corresponding question below.\n",
    "- You should train your model with the following hyper-parameters:\n",
    "```\n",
    "INPUT_DIM = 6004\n",
    "OUTPUT_DIM = 6004\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "N_LAYERS = 1\n",
    "BI_DIRECTION = True\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = XXXXX (you should figure out this hyper-parameter). \n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "TEACH_FORCING_RATE = 0.5\n",
    "LEARNING_RT = 0.001\n",
    "MAX_EPOCH = 15\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RT)\n",
    "```\n",
    "- You should use `init_weights()` function from this week tutorial to initialize model with normal distribution with `mean=0` and `std=0.01`. \n",
    "- Your seed of randomization should be 531 (i.e., manual_seed = 531). \n",
    "- You should use `nn.CrossEntropyLoss()` loss function and ignore `<pad>` tokens.\n",
    "- You should save the model checkpoint at the end of each epoch. You also need to save your vocabularies.\n",
    "- You should use a different checkpoint directory to avoid overwriting previous models. \n",
    "- Then, you load the best checkpoint and evaluate it on TEST set. \n",
    "- You should keep your vocabularies and best checkpoints. We will use them in Exercise 3 (Error analysis). \n",
    "\n",
    "**Hints:**\n",
    "- Although the encoder is bi-directional LSTM, the decoder must be a uni-directional LSTM. \n",
    "- The last hidden state of the LSTM is `h_n` of shape (num_layers * num_directions, batch, hidden_size).\n",
    "- The last cell state of the LSTM is `c_n` of shape (num_layers * num_directions, batch, hidden_size).\n",
    "- All the hidden states from the last LSTM layer is `output` of shape (seq_len, batch, num_directions * hidden_size).\n",
    "- The initialization states (i.e., $s_0$,$c_0$) of Decoder must match the dimension of Decoder. Namely, you should give a appropriate number of `DEC_HID_DIM` via analyzing the relation between tensor shapes. \n",
    "- You can use `print(XXX.shape)` to check the shape of your tensor. If the tensor shape doesn't match the desired shape of tensor, you should reshape it using `.view(), .squeeze(), .unsqueeze() or .permute() function.`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate your model evaluation, we provide a `inference()` function which calculates BLEU score based on a test corpus (test_eng_fre.tsv).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention=False, max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.data import TabularDataset\n",
    "    from torchtext.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "    sort = False,batch_size=128,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "                output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "                #original \n",
    "                #output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "                \n",
    "                # update:\n",
    "                output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inference()` function will take five variables `model, file_name, trg_vocab,attention, and max_trg_len` as inputs and return a corpus cumulative BLEU-4 score. Here is a use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "print(inference(model_best, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC, TRG, True, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please paste your code for the corresponding questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.1 Please revise the following code to build the appropriate vocabularies:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.2 You may need to revise `class Encoder`. Please show your code for `class Encoder`:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.3  You may need to revise `class Decoder`. Please show your code for `class Decoder`:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.4 You may need to revise `class Seq2Seq`. Please show your code for `class Seq2Seq`:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.5 You may need to revise the code of instantiating classes. Please show your code for instantiation:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.6 Please paste your full training log here. Which epoch is the best?\n",
    "rubric={accuracy:2}\n",
    "Your log should look like this:\n",
    "```\n",
    "Epoch: 01 | Time: 1m 25s\n",
    "\tTrain Loss: 4.293 | Train PPL:  73.188\n",
    "\t Val. Loss: 4.263 |  Val. PPL:  71.012 \n",
    "   ............\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**\n",
    "**My best model is trained with XX epochs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Your log goes here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2.7 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via corpus_bleu() function.\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here**\n",
    "\n",
    "**My best model obtains XX.XX cumulative BLEU-4 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Seq2Seq Review\n",
    "\n",
    "\n",
    "### 1.1 Warm Up\n",
    "rubric={reasoning:2}\n",
    "\n",
    "As a quick warm up. Take a minute to review the code from the tutorial and identify hyper parameters related to the three sections of code: Encoder, Decoder, and Seq2Seq. You can just copy and paste these hyper parameters into the box below.   \n",
    "\n",
    "*Note just because something is fed into the initialization function doesn't mean it's a hyperparameter (e.g. input dimension which depends on the problem), similarly there is at least one hyper-parameter which is relevant but not put in the initialization function (for better or worse).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paste the relevant hyper parameters in each section:\n",
    "```\n",
    "Encoder:\n",
    "\n",
    "Decoder:\n",
    "\n",
    "Seq2Seq:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]    \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [sequence_len, batch_size]\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Initialization\n",
    "\n",
    "\n",
    "### 2.1 seq2seq weight initialization\n",
    "rubric={accuracy:3}\n",
    "\n",
    "In the tutorial, we used a Normal distribution for our weight initialization.\n",
    "\n",
    "On the same translation task, compare how this initialization does with using a Uniform Distribution as well as initializing with weights of Zero.  Report the BLEU-4 score of each of the models based on training using these different initializations.\n",
    "\n",
    "(We can load the French and English pickle files that you saved before to save some time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#load your pickles\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/TRG.Field\",\"rb\")as f:\n",
    "     TRG = pickle.read(f)\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/SRC.Field\",\"rb\")as f:\n",
    "     SRC = pickle.read(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to change by commenting out and replacing parts\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training procedure goes here as needed. (can use the tutorial as a guide)\n",
    "#BE SURE TO USE THE SAME SEED EACH TIME YOU RUN!\n",
    "manual_seed = 531\n",
    "torch.manual_seed(manual_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in\n",
    "```\n",
    "Normal Intialization BLEU-4:\n",
    "Uniform Initialization BLEU-4:\n",
    "Zero Initialization BLEU-4:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 BLEU in the face\n",
    "\n",
    "BLEU can be a little counter-intuitive, it's not the only metric used to evaluate translations (ROUGE, METEOR etc.), but it's by far the most common. Some issues with it is that it's not really a percentage (even though it's out of 100) and is more useful to think about in terms of relative comparison to other translations in the same domain (rather than % of a sentence translated \"correctly\"). The goal of this problem is to walk you through evaluating a couple different strings to see how differences in the translation impact the final BLEU score, so that you have a more intuitive sense of what the metric means.\n",
    "\n",
    "\n",
    "#### Sidebar on BLEU\n",
    "BLEU in some domains might be very low, but still potentially useful. State of the art translation results in, for instance, low-resource spoken language translation might be single digits to at most low 20s, what's important is that the score is compared to other scores in the same domain (under the same conditions).\n",
    "\n",
    "Finally we'll just be using a single reference sentence for this example, in some cases you might have access to corpora with multiple references, this generally will lead to overall higher calculated BLEU score (but may not actually reflect an improvement in translation quality per se).\n",
    "\n",
    "### Sentence BLEU calculation\n",
    "\n",
    "For the following sentences follow the steps to compute the Sentence BLEU score by hand.  \n",
    "```\n",
    "Candidate Sentence 1: \"The the the the the the the the the the the the the the the the\" [Length: 16]\n",
    "\n",
    "Candidate Sentence 2: \"The north wind and sun are awakening, which is even stronger.\" [Length: 11]\n",
    "\n",
    "Candidate Sentence 3: \"Sun had a quarrel\" [Length: 4]\n",
    "\n",
    "Reference Sentence: \"The North Wind and the Sun had a quarrel about which of them was the stronger.\" [Length: 16]\n",
    "```\n",
    "\n",
    "#### For all of the below problems fill in the appropriate section with the fraction or decimal that corresponds to the calculated (partial) BLEU score based on the different components that make up BLEU\n",
    "\n",
    "### 3.1 Unigram Precision\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Calculate the unigram precision (e.g. how each unigram appears in the reference sentence, divided by the length of the candidate sentence) for each of these sentences. (Hint: easy to just count words that are NOT in the reference) LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Clipped Unigram Precision\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Calculate the unigram precision (as before) but clip it so that unigrams can only be counted up to the max number of times they occur in the reference sentence. LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clipped n-gram Precision\n",
    "rubric={accuracy:3}\n",
    "\n",
    "As our clipped unigram precision before, but this time calculate the clipped 2,3,and 4-gram precisions of our sentences. (HINT: There are [length - (n-1)] n-grams in a sentence.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Candidate 1:\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:\n",
    "Candidate 2:\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:\n",
    "Candidate 3:\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Brevity Penalty \n",
    "rubric={accuracy:2}\n",
    "\n",
    "BLEU gives a brevity penalty (BP) based on the length of the candidate sentence ($c$) compared to the reference sentence ($r$) as follows: for $c \\geq r$ then $BP = 1$ for $c < r$ then $BP = exp(1-r/c)$\n",
    "\n",
    "Compute the Brevity Penalty for our candidate sentences.  (Can leave as decimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Final BLEU \n",
    "rubric={accuracy:3}\n",
    "\n",
    "The final BLEU score is then calculated as such: \n",
    "$BLEU = BP * exp(\\sum_{n=1}^N w_n log p_n)$   where $w_n$ is the weight for each of the n-grams (in our case use .25 each) and $p_n$ is the clipped precision for each of the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Conceptual Questions\n",
    "\n",
    "### 4.1 Sequence Length\n",
    "rubric{reasoning:1}\n",
    "\n",
    "Seq2Seq models have flexibility in terms of the sequence length they can handle. Explain briefly. (2-3 sentences are enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response goes here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Same language?\n",
    "rubric{reasoning:1}\n",
    "\n",
    "Seq2seq models aren't just limited to translation, consider the task of simplifying sentences to make them easier to read. You could do this by training a seq2seq model on a say a parallel corpus that incorporates English Wikipedia articles aligned to Simple English Wikipedia articles. List two other applications of applying a seq2seq model that could take input in the same language as its output. Make sure you explain each application in 1-2 sentences.  (Assume you could make/find the approrpriate aligned corpuses to make this feasible) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response goes here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Bidirectional?\n",
    "rubric{reasoning:1}\n",
    "\n",
    "Would bidirectional LSTMs/RNNs work to build an Encoder/Decoder model? Why/Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response goes here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Weights\n",
    "rubric{reasoning:3}\n",
    "\n",
    "There are several different ways to set the weights of different layers in a NN. We've seen Uniform, Normal distributions as well as setting them to some constant value. [Glorot & Bengio (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?source=post_page---------------------------) investigate how the initialization of these layers can greatly impact the performance of deep neural networks. Take a few minutes to *SKIM* the abstract and then *SKIM* the bullet points in the conclusion and write a few sentences summarizing takeaways that you can use in practice.\n",
    "\n",
    "\n",
    "Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response goes here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

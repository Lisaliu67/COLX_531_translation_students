{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Goals\n",
    "- Practice training a deep learning system for machine translation task in both a high-resource and simulated low-resource setting\n",
    "- Analyze the machine translation model errors\n",
    "- Gain more familiarity with week 4 topics\n",
    "- Gain more familiarity with the code, and be able to train an MT system and run related code\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.7.0) \n",
    "- Jupyter (latest)\n",
    "- spaCy (latest)\n",
    "- nltk (latest)\n",
    "- colab access (https://colab.research.google.com)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 24, 2021, 23:59:00 (Vancouver time) [NOTE CHANGED DEADLINE]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface: Advice for Machine Translation Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in T4 we are providing you with baseline sequence-to-sequence code, for which you are welcome to deviate. We also are allowed to use the Danish training set from T4 on the individual task (Low Resource Norwegian to English), the only data requirement for this lab is that you do not obtain additional Norwegian-English data.\n",
    "\n",
    "Regardless of where you start your development, you need to follow these instructions: \n",
    "\n",
    "**1. Data and preprocessing: If you are using python directly you can use `torchtext` to load and pre-process dataset, using OpenNMT follow the instructions from last week. You are allowed to use only spaCy for tokenizing. e.g. da_core_news_sm, nb_core_news_sm, and en_core_web_sm.**\n",
    "\n",
    "Note: BLEU metric is sensitive to the tokenization used in preprocessing the text and hence we ask all the students to use the same tokenizer (i.e, spaCy). For more details, check [this paper](https://arxiv.org/pdf/1804.08771.pdf).\n",
    "\n",
    "**2. Model selection and hyper-parameter tuning. You need to select the architecture you want to use. You may need to search the optimal hyper-parameter set to improve your model performance.**\n",
    "\n",
    "Hints: You can use the validation sets (e.g. `valid.nb` and `valid.en`) to estimate your model performance. \n",
    "\n",
    "There are many possible strategies you could take to improve performance:   \n",
    "\n",
    "a. Changing vocabulary size, batch size. Using pre-training word embedding model as your embedding weights (e.g., [google news word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), [fastText](https://fasttext.cc/)).\n",
    "\n",
    "Hint: In our tutorial, we use the embedding layer with randomly initialized weights. If you initialize your embedding layer with word embedding weights from a pre-trained word embedding model from the ones listed above, you may get improvements.\n",
    "\n",
    "b. Changing the encoder/decoder model architecture, such as RNN, GRUs, LSTM, bi-RNN, bi-GRU, bi-LSTM, Transformer.\n",
    "\n",
    "c. Changing the encoder/decoder neural network structure, such as changing hidden dimension size, number of layers, dropout rate, activation function.\n",
    "\n",
    "d. Changing the training procedure, such as number of epochs, learning rate, clipping parameter, adding regularization and momentum (or RMSProp, or Adam).\n",
    "\n",
    "e. Adding attention mechanism, such as Dot-Product/Multiplicative, Concatenative/Additive or [others](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).\n",
    "\n",
    "Hint: Due to the high requirement of computational resource, we suggest you to run your experiments on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true). \n",
    "\n",
    "**3. When you get your best model on validation set, you will use this model to generate the translation of the source  sentences in the test set (`test.nb` or `test.da` depending on task) and submit your prediction.**\n",
    "\n",
    "**4. For prediction submission, please read `submission instruction`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: \n",
    "1. You can download the notebook from Colab and and submit as **`lab4_da_colab.ipyn`** or **`lab4_nb_colab.ipyn`** depending on task. \n",
    "2. If you train your model on GPU, please make sure your model, input and loss is sent to GPU using XXX.to(device) where device is `cuda`. \n",
    "3. If you want to send the GPU varibles to CPU, please use XXX.cpu() to detach from GPU. You can find more related information [here](https://pytorch.org/docs/stable/notes/cuda.html). \n",
    "4. Google colab keeps disconnecting after 30 mins automatically if you do not respond. You can find some solutions [here](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting). \n",
    "\n",
    "``Warning``: Running on CPU will be slow. \n",
    "\n",
    "If you are running experiments directly with python you can use this helper function to generate your texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write the translations to a file. this function will help you generate the submission file for the first question.\n",
    "'''\n",
    "def out_prediction(first_name, last_name,task=\"LOW_RESOURCE\",team=\"0\", save_directory, prediction_list, ):\n",
    "    \"\"\"\n",
    "    out_prediction takes four input varibles: first_name, last_name, save_directory, prediction_list\n",
    "    <first_name>, string, your first name, e.g., Tom\n",
    "    <last_name>, string, your last name, e.g., Smith\n",
    "    <task>, string either \"LOW_RESOURCE\" or \"TEAM\"\n",
    "    <team>, string, which team number, just leave as 0 for individual\n",
    "    <save_directory>, string, directory to save the submission file, e.g., ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4\n",
    "    <prediction_list>, list of string which includes all your predictions (or translations) of TEST samples\n",
    "          e.g., ['This is the translation of my first sentence','This is the translation of my second sentence',...]\n",
    "                        \n",
    "    Generate a file is named with <yourfirstname>_<yourlastname>_<task>_<team#>.txt in save directory\n",
    "    \"\"\"\n",
    "    assert(task in [\"LOW_RESOURCE\",\"TEAM\"])\n",
    "    absolute_file_path = \"{}/{}_{}_{}_{}.txt\".format(save_directory, first_name,last_name, task, str(team))\n",
    "    output_file = open(absolute_file_path,'w')\n",
    "    output_file.write(\"English (trg)\\n\")\n",
    "    for item in prediction_list:\n",
    "        output_file.write(item+\"\\n\")\n",
    "    output_file.close()\n",
    "    print(\"submission file for the first question successfully saved at %s\"%absolute_file_path)\n",
    "\n",
    "# provide your firstname and lastname as arguments to out_prediction\n",
    "out_prediction('firstname', 'lastname','TEAM', '9' './drive/My Drive/Colab Notebooks/lab4\", test_predictions)\n",
    "\n",
    "'''\n",
    "sample output:\n",
    "submission file for the first question successfully saved at ./drive/My Drive/Colab Notebooks/lab4/firstname_lastname_TEAM_9.txt\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 Machine Translation Group Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.1 Danish to English translation Team Translation Task\n",
    "rubric={accuracy:10,quality:10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to build a machine translation model that can effectively translate sentences in Danish to English. This task follows the same spirit as the Yelp review rating prediction task (a Lab4 question from Supervised learning II). \n",
    "\n",
    "We will provide ~50,000 Danish-English sentence pairs for training your model (`train set`). We also provide ~2,000 Danish-English sentence pairs for selecting the best hyperparameters of your model (`validation set`). We provide ~2,000 Danish sentences for which we are seeking the corresponding English sentences (or translations) from you (`blind test set`).\n",
    "\n",
    "In directory `./data/team_task/`, you will find the following files:\n",
    "* `T4train.da` and `T4train.en` - Train set is a pair of aligned monolingual file each containing ~50,000 lines.\n",
    "* `T4valid.da` and `T4valid.en` - Validation set is a text file containing ~2,000 lines.\n",
    "* `T4test.da`- *Blind* test set is a text file containing ~2,000 lines. \n",
    "\n",
    "\n",
    "**The performance of your submitted systems will be evaluated on English translations for each Danish sentence in TEST set (`T4test.da`).** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Check that your submission format consists of ~2000 lines of spaCy **tokenized** English sentences.\n",
    "\n",
    "2. Put your prediction txt file in this lab directory. The prediction txt file should be named with `FIRSTNAME_LASTNAME_TEAM_Team#.txt`. We will use ``Multi-BLEU.perl`` to evaluate your submission.\n",
    "\n",
    "Hint: We provide a function `out_prediction` to help you generate the submission file, or name your prediction file appropriately from OpenNMT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.2 Please clearly describe the system you submitted in T4.1 (i.e., your best model) within 300 words.\n",
    "rubric={resoning:5}\n",
    "\n",
    "Hints: \n",
    "1. Describe all the hyper-parameters of your submitted system. You may follow the structure of Development Instruction.\n",
    "2. The range of each of the hyper-parameters that you tested on (ex. Learning Rate: [0.001-0.1]) Please format this nicely!\n",
    "3. List the strategies you attempted. What strategies did work? What did not work? (this should be about a paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.3 Notebook submission \n",
    "rubric={mechanics:5}\n",
    "Please organize your group code in `lab4_da_colab.ipynb` and only keep the code that you used to train your submitted system in T4.1, your group should agree as to what to include in this notebook. Submit this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.4 Teamwork Breakdown\n",
    "rubric={raw:5}  \n",
    "#### Please describe how each team member contributed to the project. Please use a chart with each member on it to make the formatting easy to read through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Individual Translation Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Norwegian-English Simulated Low-Resource Machine Translation (Optional)\n",
    "rubric={accuracy:10, quality:10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many languages, due to various reasons, may not have ample parallel corpuses suitable for machine translation. In this situation computational linguists rely on other tricks (transfer learning, and multilingual models being two standard tricks) to make the best of the data you have. In this task we will simulate a Low-Resource setting using a small Norwegian corpus as our Source language.\n",
    "\n",
    "(Note ideally we would use an actual low-resource language for this task, however, lack of pre-trained pipeline tools would make this much more challenging, instead we opt to simulate the process with a language both supported by Spacy's Norwegian model as well as a language with several other 'high-resource' neighbors)\n",
    "\n",
    "We will provide ~5000 Norwegian-English sentence pairs for training your model (`train set`). We also provide ~2000 Norwegian-English sentence pairs for selecting the best hyperparameters of your model (`validation set`). We provide ~2000 Norwegian sentences for which we are seeking the corresponding translation in English from you (`blind test set`).\n",
    "\n",
    "In directory `./data/low_resource/`, you will find the following files:\n",
    "* `train.nb` and `train.en` - Train set is a pair of aligned monolingual text files containing ~5000 lines. \n",
    "* `valid.nb` and `valid.en` - Validation set is a text file containing ~2000 lines.\n",
    "* `test.nb`- *Blind* test set is a text file containing ~2000 Norwegian lines to be translated. \n",
    "\n",
    "\n",
    "**The performance of your submitted systems will be evaluated on translations for each Norwegian sentence in TEST set (`nb-test-onlySRC.tsv`). We will use the default BLEU score (BLEU-4 computed using the Multi-BLEU perl script included in OpenNMT's installation, NOTE there is a slight implementation difference between it and NLTK's `corpus_bleu`) calculated with the tokenized (spacy `en_core_web_sm`) target text as comparison.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check that your submission format consists of ~2000 lines of spaCy **tokenized** English sentences.\n",
    "\n",
    "2. Put your prediction txt file in this lab directory. The prediction txt file should be named with `<yourfirstname>_<yourlastname>_LOW_RESOURCE_0.txt`. \n",
    "\n",
    "Hint: We provide a function `out_prediction` to help you generate the submission file if you are doing things in python directly, otherwise rename it before turning it in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Please clearly describe the system you submitted in 1.1 (i.e., your best model) within 300 words. (optional)\n",
    "rubric={resoning:5}\n",
    "\n",
    "Hints: \n",
    "1. Describe all the hyper-parameters of your submitted system. You may follow the structure of Development Instruction.\n",
    "2. The range of each of the hyper-parameters that you tested on (ex. Learning Rate: [0.001-0.1])\n",
    "3. List the strategies you attempted. What strategies did work? What did not work? (~a paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Please organize your code in `lab4_nb_colab.ipynb` and only keep the code that you used to train your submitted system in 1.1. Submit `lab4_nb_colab.ipynb`. (optional)\n",
    "rubric={mechanics:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Analyzing the MT model errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  For this exercise, please read [this Atlantic article](https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/) and then try to identify 2-3 issues you have noticed while working with Machine Translation models in any of the labs so far in this course. Such an effort will go through the following stages:\n",
    "rubric={reasoning:6}\n",
    "* Pick any MT model you have have trained through the course.\n",
    "* Manually inspect a handful of examples.\n",
    "* Identify any 2-3 issues/problems with the model output translation. The issues can be ones that are not covered in the article as well. \n",
    "* For each issue, write a short description of what the issue is and give an example from your actual model output. 3 sentences for each issue are enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 Conceptual Questions\n",
    "## 3.1 Non-whitespace languages (research question)\n",
    "While our standard approach of tokenization works fine for many languages, some languages don't have whitespace to allow for easy tokenization (e.g. Japanese, Chinese). There are two ways to avoid problems from this, first is to tokenize using a sophisticated tool, the second way is to use a character-based neural model. As it is likely that you may run into machine translation tasks with these languages, this question is to allow you to acquaint yourself with common tools and techniques that might be useful.\n",
    "### 3.1a C/J Tokenization\n",
    "rubric={reasoning:2}\n",
    "\n",
    "There are a number of different tools that you might be able to find for tokenizing Chinese and Japanese. For one of these two languages, perform a general search to identify one tokenization tool. For that tool, identify how often its is cited in the literature by searching on google scholar. We are looking for just a rough estimate (as the engine may have missed some actual citations). [Hint: Most of these tools was also released as a paper, usually noted on the website of the tool]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1b Character-based models (Bonus Question)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "We can also avoid using these tools if we instead use a character-based approach (e.g. say our sentence is  ”我不喜欢苹果“ =\"I don't like apples\" you would break this into characters [我,不,喜,欢,苹,果] instead of 'words/phrases' [我,不,喜欢,苹果]). For the language of your choice (Chinese/Japanese), find 2 recent (2018-2020) papers in Machine Translation that use character-based approaches, briefly (3-4 sentences [not highly technical is fine] are enough) describe the model they use and share how they compare against \"word\" based approaches. \n",
    "\n",
    "Which method (Tokenize/character model) would you choose to use and why? (2-3 sentences are enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transformer Concept Questions\n",
    "### 3.2a New Model. Better model?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Identify the clear advantages of the transformer model in comparison to the original seq2seq model based on LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2b Transformer Encoder, enough?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "For many tasks (Hint: not machine translation), people have used just the Encoder side of the Transformer architecture. For instance they might take the output of the encoder and pass it directly to a linear layer (for some task). What sorts of tasks might this make sense?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

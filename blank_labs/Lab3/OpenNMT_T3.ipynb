{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW1_j_rLzjo1"
   },
   "source": [
    "# OpenNMT Tutorial and Starter Code\n",
    "(modified from the OpenNMT quickstart to work in Colab)\n",
    "\n",
    "While creating your own models from scratch is common for many tasks, often times it's useful to rely on a tool or framework to aid in this. In this exercise we're going to look at one popular NMT tool, OpenNMT, as a way to use beam search, which could be tricky to implement efficiently on your own.\n",
    "\n",
    "Finally we'll look at how to configure different models for OpenNMT including Transformer, which we'll look at in detail next week.\n",
    "\n",
    "OpenNMT, is similar to other ML frameworks in that it relies on a combination of editable .yaml files and command line tools to run the training procedure.  \n",
    "### Make sure you have the *.yml config files from the lab repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0KLPZyf4B_L"
   },
   "source": [
    "#### Due to some colab compatibility issues we will use a different version of torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R24Vt0AksrhX",
    "outputId": "0f2c8655-691b-4698-e58a-bd1e2eb76df8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.6.0+cu101\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (708.0MB)\n",
      "\u001b[K     |████████████████████████████████| 708.0MB 25kB/s \n",
      "\u001b[?25hCollecting torchvision==0.7.0+cu101\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.7.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 60.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0+cu101) (7.0.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Found existing installation: torch 1.7.1+cu101\n",
      "    Uninstalling torch-1.7.1+cu101:\n",
      "      Successfully uninstalled torch-1.7.1+cu101\n",
      "  Found existing installation: torchvision 0.8.2+cu101\n",
      "    Uninstalling torchvision-0.8.2+cu101:\n",
      "      Successfully uninstalled torchvision-0.8.2+cu101\n",
      "Successfully installed torch-1.6.0+cu101 torchvision-0.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbZmSKOA4JJh"
   },
   "source": [
    "### Next let's get OpenNMT as well as a toy English to German corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2NZIg4rnuUW",
    "outputId": "0e45223b-6536-4ded-b7b1-735f95659722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'OpenNMT-py'...\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 17074 (delta 0), reused 0 (delta 0), pack-reused 17071\u001b[K\n",
      "Receiving objects: 100% (17074/17074), 272.99 MiB | 37.78 MiB/s, done.\n",
      "Resolving deltas: 100% (12317/12317), done.\n",
      "Obtaining file:///content/OpenNMT-py\n",
      "Collecting tqdm<5,>=4.51\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/8c/f1035bd24b0e352ddba7be320abc1603fc4c9976fcda6971ed287be59164/tqdm-4.58.0-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 10.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (1.6.0+cu101)\n",
      "Collecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
      "\u001b[?25hCollecting configargparse<2,>=1.2.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/75/ca907906cd6c4c7097a037f7adaee36b3f32a08b66baed51b86d1fcc6398/ConfigArgParse-1.3.tar.gz (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (2.4.1)\n",
      "Requirement already satisfied: flask==1.1.2 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py==2.0.1) (1.1.2)\n",
      "Collecting waitress==1.4.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
      "\u001b[?25hCollecting pyyaml==5.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 9.9MB/s \n",
      "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/aa/eaf0eaee095a5d42b6ed9fa969c8a37085e059f71efb1e55cb220e245437/pyonmttok-1.24.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6MB 7.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py==2.0.1) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->OpenNMT-py==2.0.1) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.0.1) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py==2.0.1) (1.15.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 51.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.4.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.12.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.27.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.32.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (53.0.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (7.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->OpenNMT-py==2.0.1) (1.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py==2.0.1) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask==1.1.2->OpenNMT-py==2.0.1) (1.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3->OpenNMT-py==2.0.1) (3.7.4.3)\n",
      "Building wheels for collected packages: configargparse, pyyaml\n",
      "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for configargparse: filename=ConfigArgParse-1.3-cp37-none-any.whl size=19478 sha256=89c34d15dcfe09822338bddcbfe608ca72863f905c2622ba92100dd5f118dd08\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/d5/5f/3001db0714a92f771c292603ef5dada52f9efa6467f3ea2bdf\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=1cef4cbe68c0cc1195cd3ebed9ed5f6abd7d43eee97fb298b8cb88cdc49be954\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built configargparse pyyaml\n",
      "Installing collected packages: tqdm, sentencepiece, torchtext, configargparse, waitress, pyyaml, pyonmttok, OpenNMT-py\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Running setup.py develop for OpenNMT-py\n",
      "Successfully installed OpenNMT-py configargparse-1.3 pyonmttok-1.24.0 pyyaml-5.3.1 sentencepiece-0.1.95 torchtext-0.5.0 tqdm-4.58.0 waitress-1.4.4\n",
      "--2021-03-01 21:23:25--  https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.242.22\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.242.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1662081 (1.6M) [application/x-gzip]\n",
      "Saving to: ‘toy-ende.tar.gz’\n",
      "\n",
      "toy-ende.tar.gz     100%[===================>]   1.58M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2021-03-01 21:23:25 (34.7 MB/s) - ‘toy-ende.tar.gz’ saved [1662081/1662081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenNMT/OpenNMT-py.git\n",
    "!cd OpenNMT-py; pip install -e .\n",
    "!wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n",
    "!tar xf toy-ende.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKXbe2KJ4Uqr"
   },
   "source": [
    "## Processing Vocab\n",
    "\n",
    "Once we have the corpus and OpenNMT we can build the vocab we'll use. This relies on having a config file with this information laid out.\n",
    "\n",
    "Let's take a second to look at the config file we'll be using toy-ende.yml, which you should upload to Colab using the file upload on the left.\n",
    "\n",
    "The important part of the data processing are in the top parts of the yaml file:\n",
    "\n",
    "```\n",
    "# toy_en_de.yaml\n",
    "\n",
    "## Where the samples will be written\n",
    "save_data: toy-ende/run/example\n",
    "## Where the vocab(s) will be written\n",
    "src_vocab: toy-ende/run/example.vocab.src\n",
    "tgt_vocab: toy-ende/run/example.vocab.tgt\n",
    "# Prevent overwriting existing files in the folder\n",
    "overwrite: False\n",
    "\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus_1:\n",
    "        path_src: toy-ende/src-train.txt\n",
    "        path_tgt: toy-ende/tgt-train.txt\n",
    "    valid:\n",
    "        path_src: toy-ende/src-val.txt\n",
    "        path_tgt: toy-ende/tgt-val.txt\n",
    "\n",
    "\n",
    "# Vocabulary files that were just created\n",
    "src_vocab: toy-ende/run/example.vocab.src\n",
    "tgt_vocab: toy-ende/run/example.vocab.tgt\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "We specify where the data is, where to save it, as well as the vocab files corresponding to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvhOxc0XoC-Z",
    "outputId": "3d8c7819-e7a7-4b81-94c1-a659ea89557f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2021-03-01 21:26:24,710 INFO] Counter vocab from 10000 samples.\n",
      "[2021-03-01 21:26:24,710 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2021-03-01 21:26:24,720 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-03-01 21:26:24,721 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-01 21:26:25,022 INFO] Counters src:24995\n",
      "[2021-03-01 21:26:25,023 INFO] Counters tgt:35816\n"
     ]
    }
   ],
   "source": [
    "!onmt_build_vocab -config toy-ende.yml -n_sample 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyDiOm9Q5Fiz"
   },
   "source": [
    "## Training\n",
    "\n",
    "Next we will beging training with OpenNMT, again using the same config file, however, below we'll look at the relevant parts:\n",
    "\n",
    "```\n",
    "\n",
    "# Train on a single GPU\n",
    "world_size: 1\n",
    "gpu_ranks: [0]\n",
    "\n",
    "# Where to save the checkpoints\n",
    "# Note it won't actually make it to 10,000 steps because of early stopping\n",
    "save_model: toy-ende/run/model\n",
    "save_checkpoint_steps: 500\n",
    "train_steps: 10000\n",
    "valid_steps: 500\n",
    "early_stopping: 2\n",
    "\n",
    "\n",
    "# Checkpoint settings\n",
    "keep_checkpoint: 3\n",
    "seed: 531\n",
    "warmup_steps: 400\n",
    "report_every: 100\n",
    "\n",
    "# Model (note these are actually default values, but I've explicitely written them out to show how you can edit them)\n",
    "decoder_type: rnn\n",
    "encoder_type: rnn \n",
    "enc_layers: 2\n",
    "dec_layers: 2\n",
    "enc_rnn_size: 500\n",
    "dec_rnn_size: 500\n",
    "dropout: 0.3\n",
    "global_attention : dot\n",
    "\n",
    "\n",
    "# Optimizer settings\n",
    "optim: sgd\n",
    "learning_rate: 1\n",
    "\n",
    "```\n",
    "\n",
    "Here the config file covers two major things: Model checkpointing and Model Hyperparameters.\n",
    "\n",
    "Certain settings are available only for certain models, for instance you wouldn't (want to) use positional encoding for an RNN-based model, however, it is necessary for proper training of Transformers and we could include it if we added a line ```positional_encoding: 'true'```.\n",
    "\n",
    "If we wanted to know more about any of these settings, we could take a peek at the OpenNMT [train documentation](https://opennmt.net/OpenNMT-py/options/train.html)\n",
    "\n",
    "For instance for the encoder options, it shows what available models can be used:\n",
    "```\n",
    "--encoder_type, -encoder_type\n",
    "Possible choices: rnn, brnn, ggnn, mean, transformer, cnn, transformer_lm\n",
    "\n",
    "Type of encoder layer to use. Non-RNN layers are experimental. Options are [rnn|brnn|ggnn|mean|transformer|cnn|transformer_lm].\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Finally we will train our model with this configuration. (It took about 10 minutes for the small RNN model to train). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJmdN6G_qnjo",
    "outputId": "1d52b55f-0978-4ff1-eef6-b3cb5a1a0ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-02 00:42:56,005 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
      "[2021-03-02 00:42:56,005 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
      "[2021-03-02 00:42:56,005 INFO] Missing transforms field for valid data, set to default: [].\n",
      "[2021-03-02 00:42:56,005 INFO] Parsed 2 corpora from -data.\n",
      "[2021-03-02 00:42:56,006 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
      "[2021-03-02 00:42:56,006 INFO] Loading vocab from text file...\n",
      "[2021-03-02 00:42:56,006 INFO] Loading src vocabulary from toy-ende/run/example.vocab.src\n",
      "[2021-03-02 00:42:56,049 INFO] Loaded src vocab has 24995 tokens.\n",
      "[2021-03-02 00:42:56,059 INFO] Loading tgt vocabulary from toy-ende/run/example.vocab.tgt\n",
      "[2021-03-02 00:42:56,127 INFO] Loaded tgt vocab has 35816 tokens.\n",
      "[2021-03-02 00:42:56,141 INFO] Building fields with vocab in counters...\n",
      "[2021-03-02 00:42:56,207 INFO]  * tgt vocab size: 35820.\n",
      "[2021-03-02 00:42:56,237 INFO]  * src vocab size: 24997.\n",
      "[2021-03-02 00:42:56,239 INFO]  * src vocab size = 24997\n",
      "[2021-03-02 00:42:56,239 INFO]  * tgt vocab size = 35820\n",
      "[2021-03-02 00:42:56,241 INFO] Building model...\n",
      "[2021-03-02 00:42:59,826 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(24997, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(35820, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=35820, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-03-02 00:42:59,826 INFO] encoder: 16506500\n",
      "[2021-03-02 00:42:59,827 INFO] decoder: 41363820\n",
      "[2021-03-02 00:42:59,827 INFO] * number of parameters: 57870320\n",
      "[2021-03-02 00:42:59,827 INFO] Starting training on GPU: [0]\n",
      "[2021-03-02 00:42:59,827 INFO] Start training loop and validate every 500 steps...\n",
      "[2021-03-02 00:42:59,828 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-03-02 00:42:59,828 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:43:16,122 INFO] Step 100/10000; acc:   3.91; ppl: 89034.39; xent: 11.40; lr: 1.00000; 8850/8795 tok/s;     16 sec\n",
      "[2021-03-02 00:43:20,596 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:43:32,284 INFO] Step 200/10000; acc:   4.98; ppl: 9778.75; xent: 9.19; lr: 1.00000; 8831/8810 tok/s;     32 sec\n",
      "[2021-03-02 00:43:46,778 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:43:48,735 INFO] Step 300/10000; acc:   9.37; ppl: 2038.43; xent: 7.62; lr: 1.00000; 8802/8753 tok/s;     49 sec\n",
      "[2021-03-02 00:44:05,223 INFO] Step 400/10000; acc:  10.21; ppl: 1470.47; xent: 7.29; lr: 1.00000; 8840/8749 tok/s;     65 sec\n",
      "[2021-03-02 00:44:13,243 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:44:21,945 INFO] Step 500/10000; acc:  11.59; ppl: 1139.41; xent: 7.04; lr: 1.00000; 8575/8572 tok/s;     82 sec\n",
      "[2021-03-02 00:44:21,945 INFO] valid's transforms: TransformPipe()\n",
      "[2021-03-02 00:44:21,945 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:44:27,496 INFO] Validation perplexity: 1290.59\n",
      "[2021-03-02 00:44:27,497 INFO] Validation accuracy: 11.1671\n",
      "[2021-03-02 00:44:27,497 INFO] Model is improving ppl: inf --> 1290.59.\n",
      "[2021-03-02 00:44:27,497 INFO] Model is improving acc: -inf --> 11.1671.\n",
      "[2021-03-02 00:44:27,687 INFO] Saving checkpoint toy-ende/run/model_step_500.pt\n",
      "[2021-03-02 00:44:45,354 INFO] Step 600/10000; acc:  12.80; ppl: 926.92; xent: 6.83; lr: 1.00000; 6121/6099 tok/s;    106 sec\n",
      "[2021-03-02 00:44:46,556 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:45:02,214 INFO] Step 700/10000; acc:  13.39; ppl: 797.23; xent: 6.68; lr: 1.00000; 8739/8641 tok/s;    122 sec\n",
      "[2021-03-02 00:45:13,557 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:45:18,898 INFO] Step 800/10000; acc:  14.39; ppl: 675.96; xent: 6.52; lr: 1.00000; 8455/8473 tok/s;    139 sec\n",
      "[2021-03-02 00:45:36,200 INFO] Step 900/10000; acc:  14.87; ppl: 607.13; xent: 6.41; lr: 1.00000; 8304/8260 tok/s;    156 sec\n",
      "[2021-03-02 00:45:40,993 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:45:53,128 INFO] Step 1000/10000; acc:  15.47; ppl: 536.69; xent: 6.29; lr: 1.00000; 8477/8414 tok/s;    173 sec\n",
      "[2021-03-02 00:45:53,128 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:45:58,737 INFO] Validation perplexity: 961.292\n",
      "[2021-03-02 00:45:58,737 INFO] Validation accuracy: 14.0265\n",
      "[2021-03-02 00:45:58,737 INFO] Model is improving ppl: 1290.59 --> 961.292.\n",
      "[2021-03-02 00:45:58,737 INFO] Model is improving acc: 11.1671 --> 14.0265.\n",
      "[2021-03-02 00:45:58,930 INFO] Saving checkpoint toy-ende/run/model_step_1000.pt\n",
      "[2021-03-02 00:46:15,348 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:46:17,371 INFO] Step 1100/10000; acc:  15.89; ppl: 480.34; xent: 6.17; lr: 1.00000; 5952/5945 tok/s;    198 sec\n",
      "[2021-03-02 00:46:34,713 INFO] Step 1200/10000; acc:  16.60; ppl: 421.22; xent: 6.04; lr: 1.00000; 8421/8345 tok/s;    215 sec\n",
      "[2021-03-02 00:46:43,042 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:46:52,144 INFO] Step 1300/10000; acc:  17.16; ppl: 377.19; xent: 5.93; lr: 1.00000; 8225/8203 tok/s;    232 sec\n",
      "[2021-03-02 00:47:05,396 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:47:09,589 INFO] Step 1400/10000; acc:  17.85; ppl: 339.40; xent: 5.83; lr: 1.00000; 8205/8170 tok/s;    250 sec\n",
      "[2021-03-02 00:47:27,436 INFO] Step 1500/10000; acc:  18.38; ppl: 304.61; xent: 5.72; lr: 1.00000; 8264/8186 tok/s;    268 sec\n",
      "[2021-03-02 00:47:27,437 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:47:33,099 INFO] Validation perplexity: 904.417\n",
      "[2021-03-02 00:47:33,099 INFO] Validation accuracy: 14.6707\n",
      "[2021-03-02 00:47:33,100 INFO] Model is improving ppl: 961.292 --> 904.417.\n",
      "[2021-03-02 00:47:33,100 INFO] Model is improving acc: 14.0265 --> 14.6707.\n",
      "[2021-03-02 00:47:33,295 INFO] Saving checkpoint toy-ende/run/model_step_1500.pt\n",
      "[2021-03-02 00:47:40,282 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:47:51,641 INFO] Step 1600/10000; acc:  19.18; ppl: 268.08; xent: 5.59; lr: 1.00000; 5821/5829 tok/s;    292 sec\n",
      "[2021-03-02 00:48:08,648 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:48:09,301 INFO] Step 1700/10000; acc:  19.61; ppl: 246.39; xent: 5.51; lr: 1.00000; 8121/8093 tok/s;    309 sec\n",
      "[2021-03-02 00:48:26,899 INFO] Step 1800/10000; acc:  20.33; ppl: 219.45; xent: 5.39; lr: 1.00000; 8170/8091 tok/s;    327 sec\n",
      "[2021-03-02 00:48:37,059 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:48:44,821 INFO] Step 1900/10000; acc:  20.62; ppl: 199.01; xent: 5.29; lr: 1.00000; 8056/8039 tok/s;    345 sec\n",
      "[2021-03-02 00:49:02,978 INFO] Step 2000/10000; acc:  21.12; ppl: 180.79; xent: 5.20; lr: 1.00000; 8015/7973 tok/s;    363 sec\n",
      "[2021-03-02 00:49:02,979 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:49:08,616 INFO] Validation perplexity: 982.948\n",
      "[2021-03-02 00:49:08,616 INFO] Validation accuracy: 15.0269\n",
      "[2021-03-02 00:49:08,616 INFO] Stalled patience: 1/2\n",
      "[2021-03-02 00:49:08,811 INFO] Saving checkpoint toy-ende/run/model_step_2000.pt\n",
      "[2021-03-02 00:49:12,328 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:49:27,372 INFO] Step 2100/10000; acc:  21.81; ppl: 161.21; xent: 5.08; lr: 1.00000; 5901/5859 tok/s;    388 sec\n",
      "[2021-03-02 00:49:40,862 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:49:45,079 INFO] Step 2200/10000; acc:  22.11; ppl: 147.65; xent: 4.99; lr: 1.00000; 8067/8069 tok/s;    405 sec\n",
      "[2021-03-02 00:50:03,082 INFO] Step 2300/10000; acc:  22.60; ppl: 136.52; xent: 4.92; lr: 1.00000; 8181/8104 tok/s;    423 sec\n",
      "[2021-03-02 00:50:09,362 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:50:20,619 INFO] Step 2400/10000; acc:  23.38; ppl: 118.82; xent: 4.78; lr: 1.00000; 8064/8030 tok/s;    441 sec\n",
      "[2021-03-02 00:50:37,881 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:50:38,577 INFO] Step 2500/10000; acc:  23.49; ppl: 111.66; xent: 4.72; lr: 1.00000; 7958/7951 tok/s;    459 sec\n",
      "[2021-03-02 00:50:38,578 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:50:44,227 INFO] Validation perplexity: 1242.76\n",
      "[2021-03-02 00:50:44,227 INFO] Validation accuracy: 14.4363\n",
      "[2021-03-02 00:50:44,227 INFO] Decreasing patience: 1/2\n",
      "[2021-03-02 00:50:44,432 INFO] Saving checkpoint toy-ende/run/model_step_2500.pt\n",
      "[2021-03-02 00:51:03,043 INFO] Step 2600/10000; acc:  24.11; ppl: 100.26; xent: 4.61; lr: 1.00000; 5905/5856 tok/s;    483 sec\n",
      "[2021-03-02 00:51:13,188 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:51:20,964 INFO] Step 2700/10000; acc:  24.67; ppl: 91.47; xent: 4.52; lr: 1.00000; 8030/8001 tok/s;    501 sec\n",
      "[2021-03-02 00:51:36,089 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:51:38,985 INFO] Step 2800/10000; acc:  24.84; ppl: 85.99; xent: 4.45; lr: 1.00000; 8067/8028 tok/s;    519 sec\n",
      "[2021-03-02 00:51:56,936 INFO] Step 2900/10000; acc:  25.72; ppl: 77.64; xent: 4.35; lr: 1.00000; 8046/7985 tok/s;    537 sec\n",
      "[2021-03-02 00:52:04,759 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2021-03-02 00:52:14,865 INFO] Step 3000/10000; acc:  26.41; ppl: 70.01; xent: 4.25; lr: 1.00000; 7950/7961 tok/s;    555 sec\n",
      "[2021-03-02 00:52:14,865 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2021-03-02 00:52:20,633 INFO] Validation perplexity: 1637.53\n",
      "[2021-03-02 00:52:20,633 INFO] Validation accuracy: 14.7832\n",
      "[2021-03-02 00:52:20,633 INFO] Stalled patience: 0/2\n",
      "[2021-03-02 00:52:20,633 INFO] Training finished after stalled validations. Early Stop!\n",
      "[2021-03-02 00:52:20,633 INFO] Best model found at step 1500\n",
      "[2021-03-02 00:52:20,830 INFO] Saving checkpoint toy-ende/run/model_step_3000.pt\n"
     ]
    }
   ],
   "source": [
    "!onmt_train -config toy-ende.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYpT9fpCziZJ"
   },
   "source": [
    "Once our model is saved. We can use it to actually generate predictions on our output files. Our models will be saved under the ```save_model``` setting of our config file, in this case: ```toy-ende/run/model_```  Since we are only saving every 500 training steps, and keeping the past three checkpoints, we can choose from the available models. ```model_step_1000.pt``` and ```model_step_1500.pt``` and ```model_step_2000.pt```. Our early stopping indicates the best model (lowest perplexity/highest acc) of the three is 1000, but let's look at how to pick between these three using BLEU:\n",
    "\n",
    "## Translating\n",
    "\n",
    "To do so we will need to translate the source sentences, decoding with Beam search, in this case we've chosen a ```-beam_size``` of 10, however you will be asked in the question to adjust it to different sizes.\n",
    "\n",
    "Let's first create predictions for our ```_step_2000.pt```, ```_step_2500.pt``` , ```_step_3000.pt``` models (NOTE YOUR MODEL MAY HAVE STOPPED AT A DIFFERENT POINT, IN WHICH CASE USE THE APPROPRIATE 3 LAST CHECKPOINTS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myhsvgu34DAj",
    "outputId": "06550685-f851-4dd4-e6de-657ba441eca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-02 00:55:11,643 INFO] Translating shard 0.\n",
      "[2021-03-02 00:57:56,340 INFO] PRED AVG SCORE: -1.8684, PRED PPL: 6.4777\n",
      "[2021-03-02 00:58:00,669 INFO] Translating shard 0.\n",
      "[2021-03-02 01:00:08,496 INFO] PRED AVG SCORE: -1.7845, PRED PPL: 5.9568\n",
      "[2021-03-02 01:00:12,873 INFO] Translating shard 0.\n",
      "[2021-03-02 01:02:52,549 INFO] PRED AVG SCORE: -1.6705, PRED PPL: 5.3148\n"
     ]
    }
   ],
   "source": [
    "!onmt_translate -model toy-ende/run/model_step_2000.pt -src toy-ende/src-val.txt -output toy-ende/val_2000.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\n",
    "!onmt_translate -model toy-ende/run/model_step_2500.pt -src toy-ende/src-val.txt -output toy-ende/val_2500.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\n",
    "!onmt_translate -model toy-ende/run/model_step_3000.pt -src toy-ende/src-val.txt -output toy-ende/val_3000.txt -gpu 0 -beam_size 10 -seed 531 -block_ngram 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt-Z2qmTAzQa"
   },
   "source": [
    "[Note we can now manually inspect the results under val_*.txt]\n",
    "\n",
    "Finally let's calculate the BLEU scores of the outputs! We would eventually want to select the model with Highest BLEU (in our case 37 with our 2500 step model) and use this on our test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7NMugxgA34T",
    "outputId": "18918afd-c51b-48ca-c2d0-69912c49b53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use of uninitialized value in division (/) at OpenNMT-py/tools/multi-bleu.perl line 139, <STDIN> line 3000.\n",
      "BLEU = 0.00, 18.0/1.0/0.1/0.0 (BP=0.873, ratio=0.880, hyp_len=63094, ref_len=71666)\n",
      "BLEU = 0.37, 21.5/1.8/0.2/0.0 (BP=0.550, ratio=0.626, hyp_len=44858, ref_len=71666)\n",
      "BLEU = 0.32, 18.5/1.0/0.1/0.0 (BP=0.866, ratio=0.874, hyp_len=62645, ref_len=71666)\n"
     ]
    }
   ],
   "source": [
    "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_2000.txt\n",
    "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_2500.txt\n",
    "!perl  OpenNMT-py/tools/multi-bleu.perl toy-ende/tgt-val.txt < toy-ende/val_3000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylIL0MBg-NO"
   },
   "source": [
    "# Teamwork Exercise 3\n",
    "\n",
    "We have seen how OpenNMT can be used, now let's apply it to our Multi30k dataset.\n",
    "\n",
    "You can run your code in here and then download the results to submit on github.\n",
    "\n",
    "This is a Team assignment to enable students helping one another understand the different components of the OpenNMT framework and running them correctly.\n",
    "\n",
    "*You are provided with a Multi30k.yaml to fill in, be sure to submit this alongside your colab notebook and other files in the repository.*\n",
    "\n",
    "## T3.1\n",
    "\n",
    "### Build the vocab for the Multi30k En-Fr dataset\n",
    "\n",
    "While just having a vocabulary is fine for some cases, using a sub-word tokenization might help capture morphological information better.\n",
    "\n",
    "To do this, in your config file add ```transforms: [sentencepiece, filtertoolong]``` to both the training and validation corpora.\n",
    "\n",
    "Give the code you ran to build the vocab as well as the \"data\" section of your multi30k config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93KD6CBVjcQu"
   },
   "outputs": [],
   "source": [
    "# TODO build Multi30k Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsVLrM0Pjfn8"
   },
   "source": [
    "```\n",
    "Include changes you made to the Data saving, Corpus, and Vocab section in the Config HERE\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B05G-PaWl0L-"
   },
   "source": [
    "## T3.2\n",
    "Train Model\n",
    "\n",
    "Fill in the multi30k.yaml config to setup a seq2seq model that has a 3 layer RNN encoder 2 layer RNN decoder, MLP attention, with 20% dropout, using Adam as your optimizer.\n",
    "\n",
    "Copy and paste the changed parts of the *.yml file below along with the training command you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWcx3Uuamdil"
   },
   "outputs": [],
   "source": [
    "# TODO Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBlHa2Wlm4Q9"
   },
   "source": [
    "```\n",
    "Changes to model, and optimizer here.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6WGJkJym-y9"
   },
   "source": [
    "## T3.3\n",
    "\n",
    "Decoding\n",
    "\n",
    "Create predictions for the validation set using your saved models and select the one that has the highest BLEU. You should set beam size to 5 for each of these models.\n",
    "\n",
    "Report the BLEU on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3n0ezhfm9q9"
   },
   "outputs": [],
   "source": [
    "## Code to create predictions and calculate BLEU for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcF9qBqknZg0"
   },
   "source": [
    "## T3.4 \n",
    "\n",
    "Comparing Beam Width\n",
    "\n",
    "For your BEST model compare the peformance (Both BLEU and clocktime to run)  with the following Beam Sizes: 5 (done above), 10, 15, and 20.\n",
    "\n",
    "Give your code and outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkYdBODGoJIM"
   },
   "outputs": [],
   "source": [
    "## TODO Beam comparison"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "OpenNMT_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Seq2seq with attention\n",
    "- Evaluation metric\n",
    "\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.7.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 13, 2021, 23:59:00 (Vancouver time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed of randomization and working device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 531\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention! Tidy Submission\n",
    "\n",
    "rubric={mechanics:3}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions).\n",
    "- You should not use this notebook (i.e., Lab3.ipynb) to train your model on Colab. Colab may change the layout of the jupyter notebook, and then we cannot grade on your notebook, the system will miss your grade!\n",
    "- We provide another jupyter notebook (i.e., `Lab3_exp.ipynb`) for you. You can load `Lab3_exp.ipynb` to Colab and run your experiments in this jupyter notebook. \n",
    "- Please download `Lab3_exp.ipynb` from Colab and include it in your final submission. \n",
    "- Some comments in your code will help us grade. Please use heading, comments, and mardown notations to organize your code. \n",
    "- Please feel free to add cells in `Lab3_exp.ipynb`.\n",
    "- You don't need to submit your checkpoints.\n",
    "- You can reuse any scripts of lab tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the questions of this lab, we continue to use the English-French bilingual corpus of [Multi30k](https://github.com/multi30k/dataset) dataset that we used in lab2 and tutorials. Our task is to `translate text from French language to English language`. All your model should be trained on `train_eng_fre.tsv`, validated on `val_eng_fre.tsv`, and tested on `test_eng_fre.tsv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3 Open-NMT translation with beam decoding\n",
    "While it is completely reasonable to use PyTorch to produce models from scratch for Machine Translation, there are some tools and frameworks that can be handy, especially if you don't want to implement tricky aspects of the Machine Translation process, like efficient Beam Decoding. FAIRSeq and OpenNMT are two such frameworks, of which we will look at how to do translation with the latter.\n",
    "\n",
    "Using the provided Open-nmt ipynb in Google Colab and the corresponding config files (multi30k.yml for T3), update the configuration to train an OpenNMT model Encoder-Decoder (w/attention) model for the Multi30k En-Fr dataset. Consider downloading your final checkpoint from colab for safekeeping (you don't need to include this in your submission).\n",
    "\n",
    "OpenNMT_T3.ipynb should be configured to run in Colab and give an overview of how to use OpenNMT as well as sections for the T3 questions.\n",
    "\n",
    "Copy any cells from the Colab notebook that you used to run your model, as well as any updates to the config file that you needed to make (They should be labeled T3.1-T3.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylIL0MBg-NO"
   },
   "source": [
    "\n",
    "## T3.1\n",
    "\n",
    "### Build the vocab for the Multi30k En-Fr dataset\n",
    "rubric={accuracy:2}\n",
    "\n",
    "While just having a vocabulary is fine for some cases, using a sub-word tokenization might help capture morphological information better.\n",
    "\n",
    "\n",
    "To do this, in your config file add ```transforms: [sentencepiece, filtertoolong]``` to both the training and validation corpora.\n",
    "\n",
    "Give the code you ran to build the vocab as well as the \"data\" section of your multi30k config file.\n",
    "\n",
    "Note: Use the train.fr and train-1.en files, or be prepared to split the train_eng_fre.tsv file into two separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93KD6CBVjcQu"
   },
   "outputs": [],
   "source": [
    "# TODO build Multi30k Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsVLrM0Pjfn8"
   },
   "source": [
    "```\n",
    "Include changes you made to the Data saving, Corpus, and Vocab section in the Config HERE\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B05G-PaWl0L-"
   },
   "source": [
    "## T3.2\n",
    "### Train Model\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Fill in the multi30k.yaml config to setup a seq2seq model that has a 3 layer RNN encoder 2 layer RNN decoder, MLP attention, with 20% dropout, using Adam as your optimizer.\n",
    "\n",
    "Copy and paste the changed parts of the *.yml file below along with the training command you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWcx3Uuamdil"
   },
   "outputs": [],
   "source": [
    "# TODO Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBlHa2Wlm4Q9"
   },
   "source": [
    "```\n",
    "Changes to model, and optimizer here.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6WGJkJym-y9"
   },
   "source": [
    "## T3.3\n",
    "### Decoding\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Create predictions for the validation set using your saved models and select the one that has the highest BLEU. You should set beam size to 5 for each of these models.\n",
    "\n",
    "Report the BLEU on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3n0ezhfm9q9"
   },
   "outputs": [],
   "source": [
    "## Code to create predictions and calculate BLEU for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcF9qBqknZg0"
   },
   "source": [
    "## T3.4 \n",
    "### Comparing Beam Width\n",
    "rubric={accuracy:3}\n",
    "\n",
    "For your BEST model compare the peformance (Both BLEU and clocktime to run)  with the following Beam Sizes: 5 (done above), 10, 15, and 20.\n",
    "\n",
    "Give your code and outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkYdBODGoJIM"
   },
   "outputs": [],
   "source": [
    "## TODO Beam comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1 OpenNMT Transformer model\n",
    "\n",
    "Also in the assignment repository is a Transformer configuration file: ```config-transformer-base-1GPU.yml``` \n",
    "\n",
    "The next couple questions ask about aspects of this file, with the purpose of getting you familiar with using it in case you want to use a similar configuration for the Lab 4 challenges.\n",
    "\n",
    "### 1.1 Comparison to RNN model\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Ignoring data-processing fields, what fields in the config file seem to be changed from RNN models? Pick two of these changes, look up, and explain what they do (https://opennmt.net/OpenNMT-py/options/train.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 Transformer Config\n",
    "rubric={reasoning:1}\n",
    "\n",
    "There are a number of fields set in the config file that are obligatory or strongly recommended for running the Transformer, e.g. Glorot initialization. What are the other fields that must be set or are suggested to be set to run it correctly? (Hint you can look at https://opennmt.net/OpenNMT-py/options/train.html for a better idea of which ones are required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Size of Transformer\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Try loading the Transformer config file and training with it (you can reuse the data processing sections from either multi30k.yml or toy-ende.yml) but set the number of steps to some trivial amount (e.g. 100). As it starts training it should load, display the model, and give a size for the number of parameters.   \n",
    "EX for RNN:\n",
    "```\n",
    "[2021-03-02 00:42:59,826 INFO] encoder: 16506500\n",
    "[2021-03-02 00:42:59,827 INFO] decoder: 41363820\n",
    "[2021-03-02 00:42:59,827 INFO] * number of parameters: 57870320\n",
    "```\n",
    "Report how many total parameters are required to run the 'default' Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have four seq2seq models: (1) basic seq2seq from week2 tutorial (`seq2seq_tutorial.ipynb`); (2) seq2seq with additive attention from week3 tutorial (`attention_tutorial.ipynb`); (3) seq2seq variant in Exercise T2.1; and finally (4) Open-NMT seq2seq with MLP attention. Please use the best models of these five models to answer the following questions. \n",
    "\n",
    "If you had problems creating any of these 4 models, just report your results on the models you were able to make work/develop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via the corpus_bleu() function.\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "- For model (1), **my best model obtains XX.XX cumulative BLEU-4 score with XX epoch(s).** \n",
    "- For model (2), **my best model obtains XX.XX cumulative BLEU-4 score with XX epoch(s).** \n",
    "- For model (3), **my best model obtains XX.XX cumulative BLEU-4 score with XX epoch(s).** \n",
    "- For model (4), **my best model obtains XX.XX cumulative BLEU-4 score with XX epoch(s).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Please evaluate any one of four models on on test set (i.e., `test_eng_fre.tsv`) and report its BLEU-1, cumulative BLEU-2, BLEU-3, and BLEU-4 scores via the corpus_bleu() function.\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hints: \n",
    "- You can use the `inference()` function. But you will need to revise few lines of this function.\n",
    "- You may need to review the BLEU implementation in week 2 tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "- **I evaluate on model (X).** \n",
    "- **This model obtains XX.XX BLEU-1 score** \n",
    "- **This model obtains XX.XX cumulative BLEU-2 score** \n",
    "- **This model obtains XX.XX cumulative BLEU-3 score** \n",
    "- **This model obtains XX.XX cumulative BLEU-4 score** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Please explain the results of 2.2 briefly. \n",
    "rubric={reasoning:2}\n",
    "\n",
    "Hints: What is the relationship between BLEU-n scores? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the effects of sentence length, we create two subsets of test file: A. `long_test_eng_fre.tsv` which only includes sentence pairs that English reference is longer than 19 tokens; B. `short_test_eng_fre.tsv` which only includes sentence pairs that English reference is shorter than 9 tokens. `long_test_eng_fre.tsv` includes 67 sentence pairs.  `short_test_eng_fre.tsv` includes 85 sentence pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Please report the cumulative BLEU-4 score on long sentences of test set (i.e., `long_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `long_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains XX.XX cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains XX.XX cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains XX.XX cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains XX.XX cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Please report the cumulative BLEU-4 score on short sentences of test set (i.e., `short_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `short_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains XX.XX cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains XX.XX cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains XX.XX cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains XX.XX cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Please compare the results of questions of 2.1-2.3. Which model does perform best? What differences between their performance? How does the length of sentence affect performance?\n",
    "rubric={reasoning:4}\n",
    "\n",
    "Hint:\n",
    "- You can review the Section 5 of [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) to answer this question.\n",
    "- You answer should be less than 100 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 BLEU\n",
    "rubric={reasoning:3}\n",
    "\n",
    "BLEU is one of the most widely used metrics for NLP, but why can't we just use accuracy to measure the success of translations? Does accuracy make sense in machine translation? \n",
    "\n",
    "Two other (previously mentioned) metrics are METEOR and ROUGE. Pick one and briefly summarize how it is different than BLEU and why you might choose to use it instead of BLEU. (CITE SOURCES!)\n",
    "\n",
    "Hint: Feel free to think back to Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Teacher Forcing\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For training seq2seq models we often use Teacher Forcing as part of the training procedure. Identify two advantages of using this approach in training.  \n",
    "\n",
    "Hint: Feel free to do some research to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Attention bottleneck\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Learning attention functions automatically require large volumes of data especially for recurrent neural networks. Can you recommend a solution to overcome this problem? Barrett and Bingel investigate an approach to solve this limitation. Take a few minutes to SKIM the abstract and introduction and write a few sentences summarizing takeaways that you can use in practice.\n",
    "\n",
    "Barrett, M., Bingel, J., Hollenstein, N., Rei, M., & Søgaard, A. (2018, October). Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning (pp. 302-312).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Global Attention Mechanisms\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Global attention mechanisms consider all the encoder hidden states when deriving the context vector while local attention mechanisms consider only a subset of the encoder hidden states when deriving the context vector. [Luong et al. 2015](https://arxiv.org/pdf/1508.04025.pdf) proposed **three global attention mechanisms**. In the tutorial, we looked at (terminologies are borrowed from Luongs's paper):\n",
    "* Dot: $e_{ij} = s_{i-1}^Th_j \\in \\mathcal{R}$\n",
    "* Concat: $e_{ij} = v_a tanh( W_a [s_{i-1};h_j;]) \\in \\mathcal{R}$ where $W_a \\in \\mathcal{R}^{h\\times 2h}$ and $v_a \\in \\mathcal{R}^{1\\times h}$\n",
    "\n",
    "Can you find out the third global attention mechanism by going over the paper (especially Section 3.1) and write it down in the same format as above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
